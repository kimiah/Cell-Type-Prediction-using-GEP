{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otRdoaF61rmK"
   },
   "source": [
    "# CE-40550 Machine Learning for Bioinformatics (55 + 20 points)\n",
    "## Deadline: 30th Ordibehesht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kOjT7zU41rmL"
   },
   "source": [
    "# Task description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvkPVAPK1rmM"
   },
   "source": [
    "Gene expression profile is the measurement of the activity (the expression) of thousands of genes in a sample. The internal patterns of the GEPs such as coexpression of large clusters of genes suggest that the dimensionality of GEPs can be significantly reduced. We are going to use a variety of dimension reduction techniques to see if this can really be accomplished, and to compare these techniques' suitability for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXsw9yZT1rmO"
   },
   "source": [
    "For the purpose of this homework, we consider the reconstruction mean squared error as the measure to compare different methods. In other words, For each method, we first reduce the dimension and then try to reconstruct the input using the reduced data. Finally we compute MSE between the reconstructed and original data and use it as the measure to compare methods for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-XIEs2F1rmO"
   },
   "source": [
    "We will use a latent space of dimension 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHK6X5pc1zv7"
   },
   "outputs": [],
   "source": [
    "# Load data from google drive\n",
    "\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "# choose a local (colab) directory to store the data.\n",
    "local_download_path = os.path.expanduser('')\n",
    "try:\n",
    "  os.makedirs(local_download_path)\n",
    "except: pass\n",
    "\n",
    "# Download a file based on its file ID.\n",
    "file_id = '1vcBzIMtv_cvQ63jfVCbTQQ-v9thmCLRS'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "fname = os.path.join(local_download_path, downloaded['title'])\n",
    "# print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n",
    "\n",
    "print('title: %s, id: %s' % (downloaded['title'], downloaded['id']))\n",
    "downloaded.GetContentFile(fname)\n",
    "print(fname)\n",
    "!unzip \"dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4r2TT27g1rmQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUdTKIEc1rmU"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVl64RZr1rmX"
   },
   "source": [
    "You may download the dataset from <a href=\"https://drive.google.com/open?id=1vcBzIMtv_cvQ63jfVCbTQQ-v9thmCLRS\">here</a>. `data.txt` contains normalized expression profiles of 20184 genes in 1040 samples of 16 different human tissues and cell types from different datasets of NCBI GEO, which were collected and preprocessed in CellNet. `SampleCellTypes.txt` contains the type of the cell samples present in `data.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCFSpH9U1rmX"
   },
   "source": [
    "Extract the downloaded data and then use the code below to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_Ll7Nm71rmY"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data.txt\", header=None)\n",
    "labels = pd.read_csv(\"data/SampleCellTypes.txt\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-bWye471rma"
   },
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rYmVf-M1rmb"
   },
   "source": [
    "To reduce the computation costs, instead of working on all 20184 genes present in the dataset, we will focus on 1000 genes that have the most variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BRjYvuGp1rmc",
    "outputId": "fc7a6d55-ef12-48b2-9e61-6bf27d4b887e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 20184)\n",
      "(1040, 1000)\n"
     ]
    }
   ],
   "source": [
    "# TODO: limited_data should contain columns of data corresponding to genes with most variance\n",
    "limited_data = None\n",
    "n_features = 1000\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# find variance of features\n",
    "selector = VarianceThreshold()\n",
    "selector.fit_transform(data)\n",
    "vars = selector.variances_\n",
    "var_threshold = sorted(vars)[-(n_features + 1)]\n",
    "\n",
    "\n",
    "def variable_selection(data, threshold):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    return selector.fit_transform(data)\n",
    "\n",
    "limited_data = variable_selection(data, var_threshold)\n",
    "print(data.shape)\n",
    "print(limited_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOrDLCWu1rmf"
   },
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kiHTVJDY1rmg"
   },
   "source": [
    "You may use existing libraries to complete the following two tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Tdrwcq81rmi"
   },
   "outputs": [],
   "source": [
    "# TODO: Split the data to train and test with a ratio of 3:1.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "onehotencoder = OneHotEncoder()\n",
    "y = onehotencoder.fit_transform(labels).toarray()\n",
    "n_classes = np.size(y, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(limited_data, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrrYnl7R1rmm"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement a function that given the original data and a\n",
    "# reconstructed version, returns the reconstruction's mean squared error.\n",
    "def loss_function(a, b):\n",
    "    return np.square(np.linalg.norm(a - b, ord='fro')) / np.prod(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILKUhx6r1rmo"
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "badHm1GS1rmp"
   },
   "source": [
    "Implement PCA. Then use it to project the data to the latent space. Finally, reconstruct the data and report the reconstruction error for both training and test datasets. Note that you should implement the PCA, so using existing libraries is not allowed. However you may use libraries to compute eigenvectors/eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdzfWIUx1rmp"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the following method\n",
    "def PCA(X, mean, var, n_components=30):\n",
    "    \"\"\"\n",
    "        X: a vector of shape (N, M)\n",
    "        n_components: size of latent space (< M)\n",
    "        returns a vector of shape (n_components, M) containing the first `n_components` PCs.\n",
    "    \"\"\"\n",
    "    \n",
    "    N, M = X.shape\n",
    "    \n",
    "    # zero mean\n",
    "#     X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    X = (X - mean) / var\n",
    "    \n",
    "    # compute covariance matrix\n",
    "    C = np.dot(X.T, X)\n",
    "    \n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(C)\n",
    "    idx = np.argsort(eigen_vals)[::-1][:n_components]\n",
    "    eigen_vecs = eigen_vecs[:,idx]\n",
    "    \n",
    "    return eigen_vecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdI4UNOy1rmt"
   },
   "outputs": [],
   "source": [
    "def reconstruct_PCA(X, mean=None, var=None):\n",
    "  \n",
    "    # TODO: Compute PCs for the X.\n",
    "    principal_components = PCA(X, mean=mean, var=var)\n",
    "\n",
    "    # TODO: Project X to the space of computed PCs.\n",
    "    X = (X - mean) / var\n",
    "    projected_data = np.dot(X, principal_components.T)\n",
    "\n",
    "    # TODO: Reconstruct X from projected data.\n",
    "    reconstructed_data = np.dot(projected_data, principal_components) * var + mean\n",
    "    \n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Z38HXD3x1rm0",
    "outputId": "14421dec-1aa8-4489-85f8-ffd445cd2755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.366957\n",
      "Test error: 0.291576\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reconstruct train and test data and compute reconstruction error\n",
    "train_pca_error = None\n",
    "test_pca_error = None\n",
    "\n",
    "mean = X_train.mean(axis=0)\n",
    "var = X_train.std(axis=0)\n",
    "\n",
    "train_pca_error = loss_function(X_train, reconstruct_PCA(X_train, mean=mean, var=var))\n",
    "test_pca_error = loss_function(X_test, reconstruct_PCA(X_test, mean=mean, var=var))\n",
    "\n",
    "print(\"Train error: %f\" % train_pca_error)\n",
    "print(\"Test error: %f\" % test_pca_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkgXE7Xk1rm5"
   },
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNevQpTd1rm6"
   },
   "source": [
    "Implement auto encoder with following encoder architecture using keras.\n",
    "```\n",
    "Dense(400) with LogSigmoid as activation.\n",
    "Dense(30)\n",
    "```\n",
    "Use symmetric layers as decoder to reconstruct the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fLdOyLD1rm7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "q14JhwLP1rnD",
    "outputId": "a650bd74-7e12-4539-f184-12721d1f2b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 400)               400400    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 30)                12030     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 400)               12400     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1000)              401000    \n",
      "=================================================================\n",
      "Total params: 825,830\n",
      "Trainable params: 825,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(1000,))\n",
    "\n",
    "encoded = input_data\n",
    "# TODO: Implement encoder\n",
    "encoded = Dense(400, activation=tf.math.log_sigmoid)(encoded)\n",
    "encoded = Dense(30)(encoded)\n",
    "\n",
    "decoded = encoded\n",
    "# TODO: Implement decoder\n",
    "decoded = Dense(400, activation=tf.math.log_sigmoid)(decoded)\n",
    "decoded = Dense(1000, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_data, decoded)\n",
    "encoder = Model(input_data, encoded)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBZpMt0E1rnF"
   },
   "outputs": [],
   "source": [
    "# TODO: find suitable values for hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khupYYCd1rnI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autoencoder = Model(input_data, decoded)\n",
    "sgd = optimizers.SGD(lr=learning_rate)\n",
    "autoencoder.compile(optimizer=sgd, loss='mean_squared_error')\n",
    "\n",
    "mean = X_train.mean(axis=0)\n",
    "var = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - mean) / var\n",
    "X_test_scaled = (X_test - mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "euLElYgc1rnK",
    "outputId": "b388fe06-b4bc-4a65-de37-4c613022b9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 624 samples, validate on 156 samples\n",
      "Epoch 1/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.7181 - val_loss: 0.7072\n",
      "Epoch 2/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.7171 - val_loss: 0.7063\n",
      "Epoch 3/500\n",
      "624/624 [==============================] - 0s 540us/step - loss: 0.7161 - val_loss: 0.7054\n",
      "Epoch 4/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.7153 - val_loss: 0.7045\n",
      "Epoch 5/500\n",
      "624/624 [==============================] - 0s 586us/step - loss: 0.7144 - val_loss: 0.7034\n",
      "Epoch 6/500\n",
      "624/624 [==============================] - 0s 604us/step - loss: 0.7136 - val_loss: 0.7029\n",
      "Epoch 7/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.7127 - val_loss: 0.7018\n",
      "Epoch 8/500\n",
      "624/624 [==============================] - 0s 621us/step - loss: 0.7116 - val_loss: 0.7010\n",
      "Epoch 9/500\n",
      "624/624 [==============================] - 0s 625us/step - loss: 0.7108 - val_loss: 0.7002\n",
      "Epoch 10/500\n",
      "624/624 [==============================] - 0s 595us/step - loss: 0.7099 - val_loss: 0.6990\n",
      "Epoch 11/500\n",
      "624/624 [==============================] - 0s 595us/step - loss: 0.7088 - val_loss: 0.6984\n",
      "Epoch 12/500\n",
      "624/624 [==============================] - 0s 608us/step - loss: 0.7080 - val_loss: 0.6975\n",
      "Epoch 13/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.7073 - val_loss: 0.6968\n",
      "Epoch 14/500\n",
      "624/624 [==============================] - 0s 613us/step - loss: 0.7066 - val_loss: 0.6961\n",
      "Epoch 15/500\n",
      "624/624 [==============================] - 0s 609us/step - loss: 0.7060 - val_loss: 0.6954\n",
      "Epoch 16/500\n",
      "624/624 [==============================] - 0s 612us/step - loss: 0.7053 - val_loss: 0.6950\n",
      "Epoch 17/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.7046 - val_loss: 0.6941\n",
      "Epoch 18/500\n",
      "624/624 [==============================] - 0s 607us/step - loss: 0.7038 - val_loss: 0.6936\n",
      "Epoch 19/500\n",
      "624/624 [==============================] - 0s 599us/step - loss: 0.7032 - val_loss: 0.6928\n",
      "Epoch 20/500\n",
      "624/624 [==============================] - 0s 620us/step - loss: 0.7025 - val_loss: 0.6924\n",
      "Epoch 21/500\n",
      "624/624 [==============================] - 0s 609us/step - loss: 0.7020 - val_loss: 0.6915\n",
      "Epoch 22/500\n",
      "624/624 [==============================] - 0s 617us/step - loss: 0.7014 - val_loss: 0.6909\n",
      "Epoch 23/500\n",
      "624/624 [==============================] - 0s 604us/step - loss: 0.7008 - val_loss: 0.6903\n",
      "Epoch 24/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.7003 - val_loss: 0.6898\n",
      "Epoch 25/500\n",
      "624/624 [==============================] - 0s 580us/step - loss: 0.6998 - val_loss: 0.6892\n",
      "Epoch 26/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6993 - val_loss: 0.6887\n",
      "Epoch 27/500\n",
      "624/624 [==============================] - 0s 589us/step - loss: 0.6988 - val_loss: 0.6883\n",
      "Epoch 28/500\n",
      "624/624 [==============================] - 0s 553us/step - loss: 0.6984 - val_loss: 0.6878\n",
      "Epoch 29/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6980 - val_loss: 0.6873\n",
      "Epoch 30/500\n",
      "624/624 [==============================] - 0s 534us/step - loss: 0.6975 - val_loss: 0.6867\n",
      "Epoch 31/500\n",
      "624/624 [==============================] - 0s 545us/step - loss: 0.6971 - val_loss: 0.6863\n",
      "Epoch 32/500\n",
      "624/624 [==============================] - 0s 519us/step - loss: 0.6967 - val_loss: 0.6860\n",
      "Epoch 33/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6963 - val_loss: 0.6855\n",
      "Epoch 34/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6959 - val_loss: 0.6852\n",
      "Epoch 35/500\n",
      "624/624 [==============================] - 0s 576us/step - loss: 0.6954 - val_loss: 0.6846\n",
      "Epoch 36/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6949 - val_loss: 0.6844\n",
      "Epoch 37/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6945 - val_loss: 0.6837\n",
      "Epoch 38/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6941 - val_loss: 0.6834\n",
      "Epoch 39/500\n",
      "624/624 [==============================] - 0s 580us/step - loss: 0.6936 - val_loss: 0.6828\n",
      "Epoch 40/500\n",
      "624/624 [==============================] - 0s 577us/step - loss: 0.6933 - val_loss: 0.6822\n",
      "Epoch 41/500\n",
      "624/624 [==============================] - 0s 573us/step - loss: 0.6929 - val_loss: 0.6818\n",
      "Epoch 42/500\n",
      "624/624 [==============================] - 0s 546us/step - loss: 0.6924 - val_loss: 0.6813\n",
      "Epoch 43/500\n",
      "624/624 [==============================] - 0s 572us/step - loss: 0.6920 - val_loss: 0.6810\n",
      "Epoch 44/500\n",
      "624/624 [==============================] - 0s 578us/step - loss: 0.6917 - val_loss: 0.6806\n",
      "Epoch 45/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6913 - val_loss: 0.6801\n",
      "Epoch 46/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6910 - val_loss: 0.6799\n",
      "Epoch 47/500\n",
      "624/624 [==============================] - 0s 581us/step - loss: 0.6903 - val_loss: 0.6794\n",
      "Epoch 48/500\n",
      "624/624 [==============================] - 0s 611us/step - loss: 0.6900 - val_loss: 0.6789\n",
      "Epoch 49/500\n",
      "624/624 [==============================] - 0s 615us/step - loss: 0.6897 - val_loss: 0.6789\n",
      "Epoch 50/500\n",
      "624/624 [==============================] - 0s 597us/step - loss: 0.6895 - val_loss: 0.6783\n",
      "Epoch 51/500\n",
      "624/624 [==============================] - 0s 603us/step - loss: 0.6892 - val_loss: 0.6782\n",
      "Epoch 52/500\n",
      "624/624 [==============================] - 0s 560us/step - loss: 0.6889 - val_loss: 0.6781\n",
      "Epoch 53/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6886 - val_loss: 0.6776\n",
      "Epoch 54/500\n",
      "624/624 [==============================] - 0s 547us/step - loss: 0.6882 - val_loss: 0.6773\n",
      "Epoch 55/500\n",
      "624/624 [==============================] - 0s 552us/step - loss: 0.6879 - val_loss: 0.6768\n",
      "Epoch 56/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6875 - val_loss: 0.6765\n",
      "Epoch 57/500\n",
      "624/624 [==============================] - 0s 609us/step - loss: 0.6872 - val_loss: 0.6761\n",
      "Epoch 58/500\n",
      "624/624 [==============================] - 0s 528us/step - loss: 0.6869 - val_loss: 0.6759\n",
      "Epoch 59/500\n",
      "624/624 [==============================] - 0s 546us/step - loss: 0.6866 - val_loss: 0.6758\n",
      "Epoch 60/500\n",
      "624/624 [==============================] - 0s 549us/step - loss: 0.6863 - val_loss: 0.6753\n",
      "Epoch 61/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6859 - val_loss: 0.6750\n",
      "Epoch 62/500\n",
      "624/624 [==============================] - 0s 536us/step - loss: 0.6856 - val_loss: 0.6747\n",
      "Epoch 63/500\n",
      "624/624 [==============================] - 0s 537us/step - loss: 0.6853 - val_loss: 0.6745\n",
      "Epoch 64/500\n",
      "624/624 [==============================] - 0s 530us/step - loss: 0.6850 - val_loss: 0.6743\n",
      "Epoch 65/500\n",
      "624/624 [==============================] - 0s 537us/step - loss: 0.6848 - val_loss: 0.6741\n",
      "Epoch 66/500\n",
      "624/624 [==============================] - 0s 542us/step - loss: 0.6846 - val_loss: 0.6738\n",
      "Epoch 67/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6843 - val_loss: 0.6736\n",
      "Epoch 68/500\n",
      "624/624 [==============================] - 0s 527us/step - loss: 0.6841 - val_loss: 0.6734\n",
      "Epoch 69/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6838 - val_loss: 0.6731\n",
      "Epoch 70/500\n",
      "624/624 [==============================] - 0s 540us/step - loss: 0.6835 - val_loss: 0.6728\n",
      "Epoch 71/500\n",
      "624/624 [==============================] - 0s 524us/step - loss: 0.6833 - val_loss: 0.6727\n",
      "Epoch 72/500\n",
      "624/624 [==============================] - 0s 545us/step - loss: 0.6830 - val_loss: 0.6725\n",
      "Epoch 73/500\n",
      "624/624 [==============================] - 0s 539us/step - loss: 0.6827 - val_loss: 0.6720\n",
      "Epoch 74/500\n",
      "624/624 [==============================] - 0s 587us/step - loss: 0.6823 - val_loss: 0.6718\n",
      "Epoch 75/500\n",
      "624/624 [==============================] - 0s 575us/step - loss: 0.6821 - val_loss: 0.6715\n",
      "Epoch 76/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6819 - val_loss: 0.6714\n",
      "Epoch 77/500\n",
      "624/624 [==============================] - 0s 566us/step - loss: 0.6817 - val_loss: 0.6712\n",
      "Epoch 78/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6815 - val_loss: 0.6709\n",
      "Epoch 79/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6813 - val_loss: 0.6707\n",
      "Epoch 80/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.6811 - val_loss: 0.6708\n",
      "Epoch 81/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6809 - val_loss: 0.6704\n",
      "Epoch 82/500\n",
      "624/624 [==============================] - 0s 580us/step - loss: 0.6806 - val_loss: 0.6699\n",
      "Epoch 83/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6803 - val_loss: 0.6699\n",
      "Epoch 84/500\n",
      "624/624 [==============================] - 0s 562us/step - loss: 0.6801 - val_loss: 0.6697\n",
      "Epoch 85/500\n",
      "624/624 [==============================] - 0s 580us/step - loss: 0.6799 - val_loss: 0.6695\n",
      "Epoch 86/500\n",
      "624/624 [==============================] - 0s 623us/step - loss: 0.6796 - val_loss: 0.6694\n",
      "Epoch 87/500\n",
      "624/624 [==============================] - 0s 605us/step - loss: 0.6794 - val_loss: 0.6691\n",
      "Epoch 88/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.6790 - val_loss: 0.6688\n",
      "Epoch 89/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.6788 - val_loss: 0.6685\n",
      "Epoch 90/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.6785 - val_loss: 0.6683\n",
      "Epoch 91/500\n",
      "624/624 [==============================] - 0s 618us/step - loss: 0.6783 - val_loss: 0.6683\n",
      "Epoch 92/500\n",
      "624/624 [==============================] - 0s 601us/step - loss: 0.6782 - val_loss: 0.6680\n",
      "Epoch 93/500\n",
      "624/624 [==============================] - 0s 605us/step - loss: 0.6780 - val_loss: 0.6678\n",
      "Epoch 94/500\n",
      "624/624 [==============================] - 0s 602us/step - loss: 0.6778 - val_loss: 0.6676\n",
      "Epoch 95/500\n",
      "624/624 [==============================] - 0s 594us/step - loss: 0.6777 - val_loss: 0.6676\n",
      "Epoch 96/500\n",
      "624/624 [==============================] - 0s 608us/step - loss: 0.6775 - val_loss: 0.6675\n",
      "Epoch 97/500\n",
      "624/624 [==============================] - 0s 601us/step - loss: 0.6774 - val_loss: 0.6674\n",
      "Epoch 98/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6772 - val_loss: 0.6673\n",
      "Epoch 99/500\n",
      "624/624 [==============================] - 0s 624us/step - loss: 0.6771 - val_loss: 0.6671\n",
      "Epoch 100/500\n",
      "624/624 [==============================] - 0s 634us/step - loss: 0.6769 - val_loss: 0.6670\n",
      "Epoch 101/500\n",
      "624/624 [==============================] - 0s 622us/step - loss: 0.6768 - val_loss: 0.6669\n",
      "Epoch 102/500\n",
      "624/624 [==============================] - 0s 639us/step - loss: 0.6767 - val_loss: 0.6669\n",
      "Epoch 103/500\n",
      "624/624 [==============================] - 0s 614us/step - loss: 0.6765 - val_loss: 0.6667\n",
      "Epoch 104/500\n",
      "624/624 [==============================] - 0s 617us/step - loss: 0.6764 - val_loss: 0.6665\n",
      "Epoch 105/500\n",
      "624/624 [==============================] - 0s 584us/step - loss: 0.6763 - val_loss: 0.6665\n",
      "Epoch 106/500\n",
      "624/624 [==============================] - 0s 572us/step - loss: 0.6762 - val_loss: 0.6666\n",
      "Epoch 107/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6761 - val_loss: 0.6664\n",
      "Epoch 108/500\n",
      "624/624 [==============================] - 0s 567us/step - loss: 0.6760 - val_loss: 0.6662\n",
      "Epoch 109/500\n",
      "624/624 [==============================] - 0s 602us/step - loss: 0.6759 - val_loss: 0.6661\n",
      "Epoch 110/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6757 - val_loss: 0.6661\n",
      "Epoch 111/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6756 - val_loss: 0.6661\n",
      "Epoch 112/500\n",
      "624/624 [==============================] - 0s 589us/step - loss: 0.6755 - val_loss: 0.6659\n",
      "Epoch 113/500\n",
      "624/624 [==============================] - 0s 608us/step - loss: 0.6753 - val_loss: 0.6659\n",
      "Epoch 114/500\n",
      "624/624 [==============================] - 0s 599us/step - loss: 0.6751 - val_loss: 0.6655\n",
      "Epoch 115/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6749 - val_loss: 0.6656\n",
      "Epoch 116/500\n",
      "624/624 [==============================] - 0s 586us/step - loss: 0.6747 - val_loss: 0.6654\n",
      "Epoch 117/500\n",
      "624/624 [==============================] - 0s 604us/step - loss: 0.6746 - val_loss: 0.6653\n",
      "Epoch 118/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6745 - val_loss: 0.6652\n",
      "Epoch 119/500\n",
      "624/624 [==============================] - 0s 614us/step - loss: 0.6744 - val_loss: 0.6651\n",
      "Epoch 120/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6743 - val_loss: 0.6650\n",
      "Epoch 121/500\n",
      "624/624 [==============================] - 0s 574us/step - loss: 0.6742 - val_loss: 0.6648\n",
      "Epoch 122/500\n",
      "624/624 [==============================] - 0s 589us/step - loss: 0.6740 - val_loss: 0.6647\n",
      "Epoch 123/500\n",
      "624/624 [==============================] - 0s 591us/step - loss: 0.6739 - val_loss: 0.6646\n",
      "Epoch 124/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6738 - val_loss: 0.6646\n",
      "Epoch 125/500\n",
      "624/624 [==============================] - 0s 575us/step - loss: 0.6737 - val_loss: 0.6646\n",
      "Epoch 126/500\n",
      "624/624 [==============================] - 0s 567us/step - loss: 0.6735 - val_loss: 0.6639\n",
      "Epoch 127/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6731 - val_loss: 0.6637\n",
      "Epoch 128/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6729 - val_loss: 0.6636\n",
      "Epoch 129/500\n",
      "624/624 [==============================] - 0s 582us/step - loss: 0.6728 - val_loss: 0.6635\n",
      "Epoch 130/500\n",
      "624/624 [==============================] - 0s 562us/step - loss: 0.6727 - val_loss: 0.6635\n",
      "Epoch 131/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6726 - val_loss: 0.6633\n",
      "Epoch 132/500\n",
      "624/624 [==============================] - 0s 559us/step - loss: 0.6725 - val_loss: 0.6633\n",
      "Epoch 133/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6724 - val_loss: 0.6633\n",
      "Epoch 134/500\n",
      "624/624 [==============================] - 0s 567us/step - loss: 0.6723 - val_loss: 0.6631\n",
      "Epoch 135/500\n",
      "624/624 [==============================] - 0s 614us/step - loss: 0.6722 - val_loss: 0.6631\n",
      "Epoch 136/500\n",
      "624/624 [==============================] - 0s 609us/step - loss: 0.6721 - val_loss: 0.6630\n",
      "Epoch 137/500\n",
      "624/624 [==============================] - 0s 641us/step - loss: 0.6720 - val_loss: 0.6630\n",
      "Epoch 138/500\n",
      "624/624 [==============================] - 0s 632us/step - loss: 0.6720 - val_loss: 0.6629\n",
      "Epoch 139/500\n",
      "624/624 [==============================] - 0s 635us/step - loss: 0.6719 - val_loss: 0.6629\n",
      "Epoch 140/500\n",
      "624/624 [==============================] - 0s 625us/step - loss: 0.6717 - val_loss: 0.6626\n",
      "Epoch 141/500\n",
      "624/624 [==============================] - 0s 614us/step - loss: 0.6716 - val_loss: 0.6626\n",
      "Epoch 142/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6715 - val_loss: 0.6625\n",
      "Epoch 143/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6714 - val_loss: 0.6624\n",
      "Epoch 144/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6713 - val_loss: 0.6624\n",
      "Epoch 145/500\n",
      "624/624 [==============================] - 0s 545us/step - loss: 0.6712 - val_loss: 0.6623\n",
      "Epoch 146/500\n",
      "624/624 [==============================] - 0s 559us/step - loss: 0.6710 - val_loss: 0.6620\n",
      "Epoch 147/500\n",
      "624/624 [==============================] - 0s 578us/step - loss: 0.6708 - val_loss: 0.6619\n",
      "Epoch 148/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6707 - val_loss: 0.6616\n",
      "Epoch 149/500\n",
      "624/624 [==============================] - 0s 564us/step - loss: 0.6706 - val_loss: 0.6616\n",
      "Epoch 150/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6705 - val_loss: 0.6615\n",
      "Epoch 151/500\n",
      "624/624 [==============================] - 0s 562us/step - loss: 0.6704 - val_loss: 0.6614\n",
      "Epoch 152/500\n",
      "624/624 [==============================] - 0s 573us/step - loss: 0.6703 - val_loss: 0.6612\n",
      "Epoch 153/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6702 - val_loss: 0.6612\n",
      "Epoch 154/500\n",
      "624/624 [==============================] - 0s 566us/step - loss: 0.6701 - val_loss: 0.6610\n",
      "Epoch 155/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6700 - val_loss: 0.6610\n",
      "Epoch 156/500\n",
      "624/624 [==============================] - 0s 583us/step - loss: 0.6699 - val_loss: 0.6610\n",
      "Epoch 157/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6699 - val_loss: 0.6609\n",
      "Epoch 158/500\n",
      "624/624 [==============================] - 0s 584us/step - loss: 0.6698 - val_loss: 0.6608\n",
      "Epoch 159/500\n",
      "624/624 [==============================] - 0s 596us/step - loss: 0.6697 - val_loss: 0.6608\n",
      "Epoch 160/500\n",
      "624/624 [==============================] - 0s 581us/step - loss: 0.6696 - val_loss: 0.6606\n",
      "Epoch 161/500\n",
      "624/624 [==============================] - 0s 562us/step - loss: 0.6695 - val_loss: 0.6605\n",
      "Epoch 162/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6694 - val_loss: 0.6605\n",
      "Epoch 163/500\n",
      "624/624 [==============================] - 0s 550us/step - loss: 0.6693 - val_loss: 0.6604\n",
      "Epoch 164/500\n",
      "624/624 [==============================] - 0s 541us/step - loss: 0.6693 - val_loss: 0.6604\n",
      "Epoch 165/500\n",
      "624/624 [==============================] - 0s 580us/step - loss: 0.6692 - val_loss: 0.6603\n",
      "Epoch 166/500\n",
      "624/624 [==============================] - 0s 570us/step - loss: 0.6691 - val_loss: 0.6602\n",
      "Epoch 167/500\n",
      "624/624 [==============================] - 0s 595us/step - loss: 0.6690 - val_loss: 0.6600\n",
      "Epoch 168/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6689 - val_loss: 0.6600\n",
      "Epoch 169/500\n",
      "624/624 [==============================] - 0s 578us/step - loss: 0.6688 - val_loss: 0.6599\n",
      "Epoch 170/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6686 - val_loss: 0.6598\n",
      "Epoch 171/500\n",
      "624/624 [==============================] - 0s 573us/step - loss: 0.6686 - val_loss: 0.6597\n",
      "Epoch 172/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6685 - val_loss: 0.6596\n",
      "Epoch 173/500\n",
      "624/624 [==============================] - 0s 578us/step - loss: 0.6684 - val_loss: 0.6596\n",
      "Epoch 174/500\n",
      "624/624 [==============================] - 0s 602us/step - loss: 0.6683 - val_loss: 0.6595\n",
      "Epoch 175/500\n",
      "624/624 [==============================] - 0s 611us/step - loss: 0.6682 - val_loss: 0.6594\n",
      "Epoch 176/500\n",
      "624/624 [==============================] - 0s 571us/step - loss: 0.6681 - val_loss: 0.6594\n",
      "Epoch 177/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6679 - val_loss: 0.6592\n",
      "Epoch 178/500\n",
      "624/624 [==============================] - 0s 565us/step - loss: 0.6678 - val_loss: 0.6590\n",
      "Epoch 179/500\n",
      "624/624 [==============================] - 0s 544us/step - loss: 0.6678 - val_loss: 0.6590\n",
      "Epoch 180/500\n",
      "624/624 [==============================] - 0s 570us/step - loss: 0.6677 - val_loss: 0.6591\n",
      "Epoch 181/500\n",
      "624/624 [==============================] - 0s 559us/step - loss: 0.6676 - val_loss: 0.6589\n",
      "Epoch 182/500\n",
      "624/624 [==============================] - 0s 578us/step - loss: 0.6675 - val_loss: 0.6589\n",
      "Epoch 183/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6675 - val_loss: 0.6589\n",
      "Epoch 184/500\n",
      "624/624 [==============================] - 0s 564us/step - loss: 0.6674 - val_loss: 0.6588\n",
      "Epoch 185/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6673 - val_loss: 0.6587\n",
      "Epoch 186/500\n",
      "624/624 [==============================] - 0s 582us/step - loss: 0.6672 - val_loss: 0.6587\n",
      "Epoch 187/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6671 - val_loss: 0.6586\n",
      "Epoch 188/500\n",
      "624/624 [==============================] - 0s 568us/step - loss: 0.6671 - val_loss: 0.6585\n",
      "Epoch 189/500\n",
      "624/624 [==============================] - 0s 562us/step - loss: 0.6670 - val_loss: 0.6586\n",
      "Epoch 190/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6669 - val_loss: 0.6586\n",
      "Epoch 191/500\n",
      "624/624 [==============================] - 0s 559us/step - loss: 0.6669 - val_loss: 0.6584\n",
      "Epoch 192/500\n",
      "624/624 [==============================] - 0s 569us/step - loss: 0.6668 - val_loss: 0.6584\n",
      "Epoch 193/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6668 - val_loss: 0.6584\n",
      "Epoch 194/500\n",
      "624/624 [==============================] - 0s 584us/step - loss: 0.6667 - val_loss: 0.6583\n",
      "Epoch 195/500\n",
      "624/624 [==============================] - 0s 567us/step - loss: 0.6666 - val_loss: 0.6583\n",
      "Epoch 196/500\n",
      "624/624 [==============================] - 0s 544us/step - loss: 0.6666 - val_loss: 0.6582\n",
      "Epoch 197/500\n",
      "624/624 [==============================] - 0s 540us/step - loss: 0.6665 - val_loss: 0.6583\n",
      "Epoch 198/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6664 - val_loss: 0.6582\n",
      "Epoch 199/500\n",
      "624/624 [==============================] - 0s 552us/step - loss: 0.6664 - val_loss: 0.6580\n",
      "Epoch 200/500\n",
      "624/624 [==============================] - 0s 548us/step - loss: 0.6662 - val_loss: 0.6580\n",
      "Epoch 201/500\n",
      "624/624 [==============================] - 0s 571us/step - loss: 0.6660 - val_loss: 0.6579\n",
      "Epoch 202/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6660 - val_loss: 0.6579\n",
      "Epoch 203/500\n",
      "624/624 [==============================] - 0s 578us/step - loss: 0.6659 - val_loss: 0.6577\n",
      "Epoch 204/500\n",
      "624/624 [==============================] - 0s 562us/step - loss: 0.6658 - val_loss: 0.6576\n",
      "Epoch 205/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6657 - val_loss: 0.6576\n",
      "Epoch 206/500\n",
      "624/624 [==============================] - 0s 560us/step - loss: 0.6656 - val_loss: 0.6576\n",
      "Epoch 207/500\n",
      "624/624 [==============================] - 0s 566us/step - loss: 0.6656 - val_loss: 0.6576\n",
      "Epoch 208/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6655 - val_loss: 0.6573\n",
      "Epoch 209/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6653 - val_loss: 0.6572\n",
      "Epoch 210/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6652 - val_loss: 0.6572\n",
      "Epoch 211/500\n",
      "624/624 [==============================] - 0s 574us/step - loss: 0.6651 - val_loss: 0.6570\n",
      "Epoch 212/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6650 - val_loss: 0.6569\n",
      "Epoch 213/500\n",
      "624/624 [==============================] - 0s 552us/step - loss: 0.6649 - val_loss: 0.6569\n",
      "Epoch 214/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6648 - val_loss: 0.6569\n",
      "Epoch 215/500\n",
      "624/624 [==============================] - 0s 589us/step - loss: 0.6647 - val_loss: 0.6567\n",
      "Epoch 216/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6647 - val_loss: 0.6568\n",
      "Epoch 217/500\n",
      "624/624 [==============================] - 0s 577us/step - loss: 0.6646 - val_loss: 0.6567\n",
      "Epoch 218/500\n",
      "624/624 [==============================] - 0s 571us/step - loss: 0.6646 - val_loss: 0.6567\n",
      "Epoch 219/500\n",
      "624/624 [==============================] - 0s 576us/step - loss: 0.6645 - val_loss: 0.6566\n",
      "Epoch 220/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.6644 - val_loss: 0.6567\n",
      "Epoch 221/500\n",
      "624/624 [==============================] - 0s 608us/step - loss: 0.6643 - val_loss: 0.6565\n",
      "Epoch 222/500\n",
      "624/624 [==============================] - 0s 603us/step - loss: 0.6643 - val_loss: 0.6563\n",
      "Epoch 223/500\n",
      "624/624 [==============================] - 0s 594us/step - loss: 0.6642 - val_loss: 0.6563\n",
      "Epoch 224/500\n",
      "624/624 [==============================] - 0s 602us/step - loss: 0.6641 - val_loss: 0.6563\n",
      "Epoch 225/500\n",
      "624/624 [==============================] - 0s 585us/step - loss: 0.6640 - val_loss: 0.6563\n",
      "Epoch 226/500\n",
      "624/624 [==============================] - 0s 603us/step - loss: 0.6640 - val_loss: 0.6562\n",
      "Epoch 227/500\n",
      "624/624 [==============================] - 0s 602us/step - loss: 0.6639 - val_loss: 0.6561\n",
      "Epoch 228/500\n",
      "624/624 [==============================] - 0s 619us/step - loss: 0.6638 - val_loss: 0.6562\n",
      "Epoch 229/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6638 - val_loss: 0.6561\n",
      "Epoch 230/500\n",
      "624/624 [==============================] - 0s 591us/step - loss: 0.6637 - val_loss: 0.6561\n",
      "Epoch 231/500\n",
      "624/624 [==============================] - 0s 577us/step - loss: 0.6637 - val_loss: 0.6561\n",
      "Epoch 232/500\n",
      "624/624 [==============================] - 0s 586us/step - loss: 0.6636 - val_loss: 0.6560\n",
      "Epoch 233/500\n",
      "624/624 [==============================] - 0s 596us/step - loss: 0.6635 - val_loss: 0.6559\n",
      "Epoch 234/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6633 - val_loss: 0.6555\n",
      "Epoch 235/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6630 - val_loss: 0.6554\n",
      "Epoch 236/500\n",
      "624/624 [==============================] - 0s 594us/step - loss: 0.6629 - val_loss: 0.6553\n",
      "Epoch 237/500\n",
      "624/624 [==============================] - 0s 591us/step - loss: 0.6628 - val_loss: 0.6552\n",
      "Epoch 238/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6628 - val_loss: 0.6552\n",
      "Epoch 239/500\n",
      "624/624 [==============================] - 0s 587us/step - loss: 0.6627 - val_loss: 0.6552\n",
      "Epoch 240/500\n",
      "624/624 [==============================] - 0s 565us/step - loss: 0.6626 - val_loss: 0.6552\n",
      "Epoch 241/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6625 - val_loss: 0.6550\n",
      "Epoch 242/500\n",
      "624/624 [==============================] - 0s 574us/step - loss: 0.6625 - val_loss: 0.6550\n",
      "Epoch 243/500\n",
      "624/624 [==============================] - 0s 574us/step - loss: 0.6624 - val_loss: 0.6550\n",
      "Epoch 244/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6623 - val_loss: 0.6549\n",
      "Epoch 245/500\n",
      "624/624 [==============================] - 0s 580us/step - loss: 0.6623 - val_loss: 0.6549\n",
      "Epoch 246/500\n",
      "624/624 [==============================] - 0s 609us/step - loss: 0.6623 - val_loss: 0.6549\n",
      "Epoch 247/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6622 - val_loss: 0.6548\n",
      "Epoch 248/500\n",
      "624/624 [==============================] - 0s 569us/step - loss: 0.6621 - val_loss: 0.6547\n",
      "Epoch 249/500\n",
      "624/624 [==============================] - 0s 598us/step - loss: 0.6621 - val_loss: 0.6547\n",
      "Epoch 250/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.6620 - val_loss: 0.6547\n",
      "Epoch 251/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6619 - val_loss: 0.6545\n",
      "Epoch 252/500\n",
      "624/624 [==============================] - 0s 580us/step - loss: 0.6619 - val_loss: 0.6545\n",
      "Epoch 253/500\n",
      "624/624 [==============================] - 0s 574us/step - loss: 0.6618 - val_loss: 0.6544\n",
      "Epoch 254/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6615 - val_loss: 0.6541\n",
      "Epoch 255/500\n",
      "624/624 [==============================] - 0s 595us/step - loss: 0.6614 - val_loss: 0.6541\n",
      "Epoch 256/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6613 - val_loss: 0.6541\n",
      "Epoch 257/500\n",
      "624/624 [==============================] - 0s 566us/step - loss: 0.6612 - val_loss: 0.6540\n",
      "Epoch 258/500\n",
      "624/624 [==============================] - 0s 581us/step - loss: 0.6612 - val_loss: 0.6539\n",
      "Epoch 259/500\n",
      "624/624 [==============================] - 0s 566us/step - loss: 0.6611 - val_loss: 0.6538\n",
      "Epoch 260/500\n",
      "624/624 [==============================] - 0s 564us/step - loss: 0.6609 - val_loss: 0.6535\n",
      "Epoch 261/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6606 - val_loss: 0.6534\n",
      "Epoch 262/500\n",
      "624/624 [==============================] - 0s 548us/step - loss: 0.6605 - val_loss: 0.6533\n",
      "Epoch 263/500\n",
      "624/624 [==============================] - 0s 541us/step - loss: 0.6605 - val_loss: 0.6533\n",
      "Epoch 264/500\n",
      "624/624 [==============================] - 0s 565us/step - loss: 0.6604 - val_loss: 0.6532\n",
      "Epoch 265/500\n",
      "624/624 [==============================] - 0s 567us/step - loss: 0.6604 - val_loss: 0.6531\n",
      "Epoch 266/500\n",
      "624/624 [==============================] - 0s 550us/step - loss: 0.6603 - val_loss: 0.6530\n",
      "Epoch 267/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6602 - val_loss: 0.6530\n",
      "Epoch 268/500\n",
      "624/624 [==============================] - 0s 577us/step - loss: 0.6602 - val_loss: 0.6529\n",
      "Epoch 269/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6601 - val_loss: 0.6528\n",
      "Epoch 270/500\n",
      "624/624 [==============================] - 0s 547us/step - loss: 0.6601 - val_loss: 0.6528\n",
      "Epoch 271/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6600 - val_loss: 0.6528\n",
      "Epoch 272/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6600 - val_loss: 0.6527\n",
      "Epoch 273/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6599 - val_loss: 0.6527\n",
      "Epoch 274/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6599 - val_loss: 0.6526\n",
      "Epoch 275/500\n",
      "624/624 [==============================] - 0s 559us/step - loss: 0.6598 - val_loss: 0.6526\n",
      "Epoch 276/500\n",
      "624/624 [==============================] - 0s 550us/step - loss: 0.6598 - val_loss: 0.6527\n",
      "Epoch 277/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6597 - val_loss: 0.6525\n",
      "Epoch 278/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6597 - val_loss: 0.6525\n",
      "Epoch 279/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6597 - val_loss: 0.6524\n",
      "Epoch 280/500\n",
      "624/624 [==============================] - 0s 572us/step - loss: 0.6596 - val_loss: 0.6525\n",
      "Epoch 281/500\n",
      "624/624 [==============================] - 0s 585us/step - loss: 0.6596 - val_loss: 0.6524\n",
      "Epoch 282/500\n",
      "624/624 [==============================] - 0s 585us/step - loss: 0.6595 - val_loss: 0.6524\n",
      "Epoch 283/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6595 - val_loss: 0.6524\n",
      "Epoch 284/500\n",
      "624/624 [==============================] - 0s 582us/step - loss: 0.6594 - val_loss: 0.6523\n",
      "Epoch 285/500\n",
      "624/624 [==============================] - 0s 601us/step - loss: 0.6594 - val_loss: 0.6522\n",
      "Epoch 286/500\n",
      "624/624 [==============================] - 0s 635us/step - loss: 0.6594 - val_loss: 0.6522\n",
      "Epoch 287/500\n",
      "624/624 [==============================] - 0s 577us/step - loss: 0.6593 - val_loss: 0.6522\n",
      "Epoch 288/500\n",
      "624/624 [==============================] - 0s 582us/step - loss: 0.6593 - val_loss: 0.6521\n",
      "Epoch 289/500\n",
      "624/624 [==============================] - 0s 612us/step - loss: 0.6592 - val_loss: 0.6521\n",
      "Epoch 290/500\n",
      "624/624 [==============================] - 0s 611us/step - loss: 0.6592 - val_loss: 0.6520\n",
      "Epoch 291/500\n",
      "624/624 [==============================] - 0s 595us/step - loss: 0.6591 - val_loss: 0.6520\n",
      "Epoch 292/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6591 - val_loss: 0.6520\n",
      "Epoch 293/500\n",
      "624/624 [==============================] - 0s 624us/step - loss: 0.6591 - val_loss: 0.6519\n",
      "Epoch 294/500\n",
      "624/624 [==============================] - 0s 624us/step - loss: 0.6590 - val_loss: 0.6519\n",
      "Epoch 295/500\n",
      "624/624 [==============================] - 0s 606us/step - loss: 0.6590 - val_loss: 0.6517\n",
      "Epoch 296/500\n",
      "624/624 [==============================] - 0s 599us/step - loss: 0.6586 - val_loss: 0.6516\n",
      "Epoch 297/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6586 - val_loss: 0.6516\n",
      "Epoch 298/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6585 - val_loss: 0.6516\n",
      "Epoch 299/500\n",
      "624/624 [==============================] - 0s 568us/step - loss: 0.6585 - val_loss: 0.6515\n",
      "Epoch 300/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6585 - val_loss: 0.6515\n",
      "Epoch 301/500\n",
      "624/624 [==============================] - 0s 618us/step - loss: 0.6584 - val_loss: 0.6514\n",
      "Epoch 302/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6583 - val_loss: 0.6513\n",
      "Epoch 303/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6583 - val_loss: 0.6513\n",
      "Epoch 304/500\n",
      "624/624 [==============================] - 0s 583us/step - loss: 0.6582 - val_loss: 0.6513\n",
      "Epoch 305/500\n",
      "624/624 [==============================] - 0s 584us/step - loss: 0.6582 - val_loss: 0.6512\n",
      "Epoch 306/500\n",
      "624/624 [==============================] - 0s 570us/step - loss: 0.6581 - val_loss: 0.6512\n",
      "Epoch 307/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6581 - val_loss: 0.6511\n",
      "Epoch 308/500\n",
      "624/624 [==============================] - 0s 550us/step - loss: 0.6580 - val_loss: 0.6512\n",
      "Epoch 309/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6580 - val_loss: 0.6510\n",
      "Epoch 310/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6579 - val_loss: 0.6511\n",
      "Epoch 311/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6579 - val_loss: 0.6510\n",
      "Epoch 312/500\n",
      "624/624 [==============================] - 0s 542us/step - loss: 0.6579 - val_loss: 0.6510\n",
      "Epoch 313/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6578 - val_loss: 0.6509\n",
      "Epoch 314/500\n",
      "624/624 [==============================] - 0s 571us/step - loss: 0.6578 - val_loss: 0.6509\n",
      "Epoch 315/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6577 - val_loss: 0.6510\n",
      "Epoch 316/500\n",
      "624/624 [==============================] - 0s 575us/step - loss: 0.6577 - val_loss: 0.6508\n",
      "Epoch 317/500\n",
      "624/624 [==============================] - 0s 550us/step - loss: 0.6577 - val_loss: 0.6508\n",
      "Epoch 318/500\n",
      "624/624 [==============================] - 0s 538us/step - loss: 0.6576 - val_loss: 0.6508\n",
      "Epoch 319/500\n",
      "624/624 [==============================] - 0s 540us/step - loss: 0.6576 - val_loss: 0.6507\n",
      "Epoch 320/500\n",
      "624/624 [==============================] - 0s 559us/step - loss: 0.6575 - val_loss: 0.6507\n",
      "Epoch 321/500\n",
      "624/624 [==============================] - 0s 545us/step - loss: 0.6575 - val_loss: 0.6508\n",
      "Epoch 322/500\n",
      "624/624 [==============================] - 0s 574us/step - loss: 0.6574 - val_loss: 0.6506\n",
      "Epoch 323/500\n",
      "624/624 [==============================] - 0s 548us/step - loss: 0.6574 - val_loss: 0.6506\n",
      "Epoch 324/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6574 - val_loss: 0.6506\n",
      "Epoch 325/500\n",
      "624/624 [==============================] - 0s 547us/step - loss: 0.6573 - val_loss: 0.6506\n",
      "Epoch 326/500\n",
      "624/624 [==============================] - 0s 536us/step - loss: 0.6573 - val_loss: 0.6504\n",
      "Epoch 327/500\n",
      "624/624 [==============================] - 0s 545us/step - loss: 0.6572 - val_loss: 0.6505\n",
      "Epoch 328/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6572 - val_loss: 0.6504\n",
      "Epoch 329/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6571 - val_loss: 0.6504\n",
      "Epoch 330/500\n",
      "624/624 [==============================] - 0s 554us/step - loss: 0.6571 - val_loss: 0.6504\n",
      "Epoch 331/500\n",
      "624/624 [==============================] - 0s 570us/step - loss: 0.6571 - val_loss: 0.6503\n",
      "Epoch 332/500\n",
      "624/624 [==============================] - 0s 560us/step - loss: 0.6571 - val_loss: 0.6503\n",
      "Epoch 333/500\n",
      "624/624 [==============================] - 0s 573us/step - loss: 0.6570 - val_loss: 0.6504\n",
      "Epoch 334/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6570 - val_loss: 0.6503\n",
      "Epoch 335/500\n",
      "624/624 [==============================] - 0s 597us/step - loss: 0.6569 - val_loss: 0.6503\n",
      "Epoch 336/500\n",
      "624/624 [==============================] - 0s 591us/step - loss: 0.6569 - val_loss: 0.6502\n",
      "Epoch 337/500\n",
      "624/624 [==============================] - 0s 604us/step - loss: 0.6568 - val_loss: 0.6503\n",
      "Epoch 338/500\n",
      "624/624 [==============================] - 0s 603us/step - loss: 0.6568 - val_loss: 0.6502\n",
      "Epoch 339/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6568 - val_loss: 0.6502\n",
      "Epoch 340/500\n",
      "624/624 [==============================] - 0s 596us/step - loss: 0.6568 - val_loss: 0.6502\n",
      "Epoch 341/500\n",
      "624/624 [==============================] - 0s 586us/step - loss: 0.6567 - val_loss: 0.6501\n",
      "Epoch 342/500\n",
      "624/624 [==============================] - 0s 585us/step - loss: 0.6567 - val_loss: 0.6501\n",
      "Epoch 343/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6566 - val_loss: 0.6500\n",
      "Epoch 344/500\n",
      "624/624 [==============================] - 0s 626us/step - loss: 0.6566 - val_loss: 0.6500\n",
      "Epoch 345/500\n",
      "624/624 [==============================] - 0s 549us/step - loss: 0.6566 - val_loss: 0.6500\n",
      "Epoch 346/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6565 - val_loss: 0.6501\n",
      "Epoch 347/500\n",
      "624/624 [==============================] - 0s 566us/step - loss: 0.6565 - val_loss: 0.6499\n",
      "Epoch 348/500\n",
      "624/624 [==============================] - 0s 591us/step - loss: 0.6565 - val_loss: 0.6499\n",
      "Epoch 349/500\n",
      "624/624 [==============================] - 0s 570us/step - loss: 0.6565 - val_loss: 0.6500\n",
      "Epoch 350/500\n",
      "624/624 [==============================] - 0s 574us/step - loss: 0.6564 - val_loss: 0.6499\n",
      "Epoch 351/500\n",
      "624/624 [==============================] - 0s 582us/step - loss: 0.6564 - val_loss: 0.6499\n",
      "Epoch 352/500\n",
      "624/624 [==============================] - 0s 573us/step - loss: 0.6564 - val_loss: 0.6498\n",
      "Epoch 353/500\n",
      "624/624 [==============================] - 0s 571us/step - loss: 0.6563 - val_loss: 0.6498\n",
      "Epoch 354/500\n",
      "624/624 [==============================] - 0s 598us/step - loss: 0.6563 - val_loss: 0.6498\n",
      "Epoch 355/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6563 - val_loss: 0.6497\n",
      "Epoch 356/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6563 - val_loss: 0.6497\n",
      "Epoch 357/500\n",
      "624/624 [==============================] - 0s 552us/step - loss: 0.6562 - val_loss: 0.6497\n",
      "Epoch 358/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6562 - val_loss: 0.6496\n",
      "Epoch 359/500\n",
      "624/624 [==============================] - 0s 567us/step - loss: 0.6562 - val_loss: 0.6496\n",
      "Epoch 360/500\n",
      "624/624 [==============================] - 0s 549us/step - loss: 0.6561 - val_loss: 0.6496\n",
      "Epoch 361/500\n",
      "624/624 [==============================] - 0s 582us/step - loss: 0.6561 - val_loss: 0.6496\n",
      "Epoch 362/500\n",
      "624/624 [==============================] - 0s 568us/step - loss: 0.6561 - val_loss: 0.6495\n",
      "Epoch 363/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6561 - val_loss: 0.6496\n",
      "Epoch 364/500\n",
      "624/624 [==============================] - 0s 586us/step - loss: 0.6560 - val_loss: 0.6495\n",
      "Epoch 365/500\n",
      "624/624 [==============================] - 0s 572us/step - loss: 0.6560 - val_loss: 0.6495\n",
      "Epoch 366/500\n",
      "624/624 [==============================] - 0s 572us/step - loss: 0.6559 - val_loss: 0.6495\n",
      "Epoch 367/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6560 - val_loss: 0.6494\n",
      "Epoch 368/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6559 - val_loss: 0.6494\n",
      "Epoch 369/500\n",
      "624/624 [==============================] - 0s 599us/step - loss: 0.6559 - val_loss: 0.6494\n",
      "Epoch 370/500\n",
      "624/624 [==============================] - 0s 619us/step - loss: 0.6558 - val_loss: 0.6494\n",
      "Epoch 371/500\n",
      "624/624 [==============================] - 0s 605us/step - loss: 0.6558 - val_loss: 0.6493\n",
      "Epoch 372/500\n",
      "624/624 [==============================] - 0s 621us/step - loss: 0.6558 - val_loss: 0.6494\n",
      "Epoch 373/500\n",
      "624/624 [==============================] - 0s 649us/step - loss: 0.6558 - val_loss: 0.6493\n",
      "Epoch 374/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6557 - val_loss: 0.6493\n",
      "Epoch 375/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6557 - val_loss: 0.6492\n",
      "Epoch 376/500\n",
      "624/624 [==============================] - 0s 568us/step - loss: 0.6557 - val_loss: 0.6492\n",
      "Epoch 377/500\n",
      "624/624 [==============================] - 0s 575us/step - loss: 0.6557 - val_loss: 0.6492\n",
      "Epoch 378/500\n",
      "624/624 [==============================] - 0s 569us/step - loss: 0.6556 - val_loss: 0.6491\n",
      "Epoch 379/500\n",
      "624/624 [==============================] - 0s 546us/step - loss: 0.6556 - val_loss: 0.6491\n",
      "Epoch 380/500\n",
      "624/624 [==============================] - 0s 587us/step - loss: 0.6556 - val_loss: 0.6492\n",
      "Epoch 381/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6555 - val_loss: 0.6491\n",
      "Epoch 382/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6555 - val_loss: 0.6491\n",
      "Epoch 383/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6555 - val_loss: 0.6491\n",
      "Epoch 384/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6554 - val_loss: 0.6491\n",
      "Epoch 385/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6554 - val_loss: 0.6490\n",
      "Epoch 386/500\n",
      "624/624 [==============================] - 0s 564us/step - loss: 0.6554 - val_loss: 0.6490\n",
      "Epoch 387/500\n",
      "624/624 [==============================] - 0s 585us/step - loss: 0.6554 - val_loss: 0.6489\n",
      "Epoch 388/500\n",
      "624/624 [==============================] - 0s 587us/step - loss: 0.6553 - val_loss: 0.6490\n",
      "Epoch 389/500\n",
      "624/624 [==============================] - 0s 586us/step - loss: 0.6553 - val_loss: 0.6489\n",
      "Epoch 390/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6553 - val_loss: 0.6489\n",
      "Epoch 391/500\n",
      "624/624 [==============================] - 0s 576us/step - loss: 0.6552 - val_loss: 0.6488\n",
      "Epoch 392/500\n",
      "624/624 [==============================] - 0s 597us/step - loss: 0.6549 - val_loss: 0.6485\n",
      "Epoch 393/500\n",
      "624/624 [==============================] - 0s 586us/step - loss: 0.6547 - val_loss: 0.6484\n",
      "Epoch 394/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6547 - val_loss: 0.6484\n",
      "Epoch 395/500\n",
      "624/624 [==============================] - 0s 562us/step - loss: 0.6547 - val_loss: 0.6484\n",
      "Epoch 396/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6546 - val_loss: 0.6483\n",
      "Epoch 397/500\n",
      "624/624 [==============================] - 0s 588us/step - loss: 0.6546 - val_loss: 0.6483\n",
      "Epoch 398/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6545 - val_loss: 0.6484\n",
      "Epoch 399/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6545 - val_loss: 0.6484\n",
      "Epoch 400/500\n",
      "624/624 [==============================] - 0s 573us/step - loss: 0.6545 - val_loss: 0.6483\n",
      "Epoch 401/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6545 - val_loss: 0.6483\n",
      "Epoch 402/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6544 - val_loss: 0.6483\n",
      "Epoch 403/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6544 - val_loss: 0.6482\n",
      "Epoch 404/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6544 - val_loss: 0.6482\n",
      "Epoch 405/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6544 - val_loss: 0.6482\n",
      "Epoch 406/500\n",
      "624/624 [==============================] - 0s 565us/step - loss: 0.6543 - val_loss: 0.6482\n",
      "Epoch 407/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6543 - val_loss: 0.6481\n",
      "Epoch 408/500\n",
      "624/624 [==============================] - 0s 602us/step - loss: 0.6543 - val_loss: 0.6481\n",
      "Epoch 409/500\n",
      "624/624 [==============================] - 0s 594us/step - loss: 0.6543 - val_loss: 0.6481\n",
      "Epoch 410/500\n",
      "624/624 [==============================] - 0s 581us/step - loss: 0.6542 - val_loss: 0.6480\n",
      "Epoch 411/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6542 - val_loss: 0.6480\n",
      "Epoch 412/500\n",
      "624/624 [==============================] - 0s 589us/step - loss: 0.6542 - val_loss: 0.6480\n",
      "Epoch 413/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6542 - val_loss: 0.6480\n",
      "Epoch 414/500\n",
      "624/624 [==============================] - 0s 618us/step - loss: 0.6541 - val_loss: 0.6480\n",
      "Epoch 415/500\n",
      "624/624 [==============================] - 0s 615us/step - loss: 0.6541 - val_loss: 0.6480\n",
      "Epoch 416/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6541 - val_loss: 0.6480\n",
      "Epoch 417/500\n",
      "624/624 [==============================] - 0s 620us/step - loss: 0.6541 - val_loss: 0.6480\n",
      "Epoch 418/500\n",
      "624/624 [==============================] - 0s 594us/step - loss: 0.6540 - val_loss: 0.6480\n",
      "Epoch 419/500\n",
      "624/624 [==============================] - 0s 607us/step - loss: 0.6540 - val_loss: 0.6479\n",
      "Epoch 420/500\n",
      "624/624 [==============================] - 0s 591us/step - loss: 0.6540 - val_loss: 0.6479\n",
      "Epoch 421/500\n",
      "624/624 [==============================] - 0s 611us/step - loss: 0.6540 - val_loss: 0.6479\n",
      "Epoch 422/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6540 - val_loss: 0.6478\n",
      "Epoch 423/500\n",
      "624/624 [==============================] - 0s 607us/step - loss: 0.6540 - val_loss: 0.6479\n",
      "Epoch 424/500\n",
      "624/624 [==============================] - 0s 616us/step - loss: 0.6539 - val_loss: 0.6479\n",
      "Epoch 425/500\n",
      "624/624 [==============================] - 0s 616us/step - loss: 0.6539 - val_loss: 0.6479\n",
      "Epoch 426/500\n",
      "624/624 [==============================] - 0s 630us/step - loss: 0.6539 - val_loss: 0.6478\n",
      "Epoch 427/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6538 - val_loss: 0.6478\n",
      "Epoch 428/500\n",
      "624/624 [==============================] - 0s 612us/step - loss: 0.6538 - val_loss: 0.6478\n",
      "Epoch 429/500\n",
      "624/624 [==============================] - 0s 602us/step - loss: 0.6538 - val_loss: 0.6478\n",
      "Epoch 430/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6538 - val_loss: 0.6477\n",
      "Epoch 431/500\n",
      "624/624 [==============================] - 0s 645us/step - loss: 0.6538 - val_loss: 0.6478\n",
      "Epoch 432/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6537 - val_loss: 0.6478\n",
      "Epoch 433/500\n",
      "624/624 [==============================] - 0s 567us/step - loss: 0.6537 - val_loss: 0.6477\n",
      "Epoch 434/500\n",
      "624/624 [==============================] - 0s 566us/step - loss: 0.6537 - val_loss: 0.6477\n",
      "Epoch 435/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6537 - val_loss: 0.6477\n",
      "Epoch 436/500\n",
      "624/624 [==============================] - 0s 556us/step - loss: 0.6537 - val_loss: 0.6476\n",
      "Epoch 437/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6536 - val_loss: 0.6477\n",
      "Epoch 438/500\n",
      "624/624 [==============================] - 0s 592us/step - loss: 0.6536 - val_loss: 0.6476\n",
      "Epoch 439/500\n",
      "624/624 [==============================] - 0s 572us/step - loss: 0.6536 - val_loss: 0.6476\n",
      "Epoch 440/500\n",
      "624/624 [==============================] - 0s 585us/step - loss: 0.6536 - val_loss: 0.6476\n",
      "Epoch 441/500\n",
      "624/624 [==============================] - 0s 587us/step - loss: 0.6536 - val_loss: 0.6477\n",
      "Epoch 442/500\n",
      "624/624 [==============================] - 0s 563us/step - loss: 0.6535 - val_loss: 0.6476\n",
      "Epoch 443/500\n",
      "624/624 [==============================] - 0s 596us/step - loss: 0.6535 - val_loss: 0.6476\n",
      "Epoch 444/500\n",
      "624/624 [==============================] - 0s 560us/step - loss: 0.6535 - val_loss: 0.6476\n",
      "Epoch 445/500\n",
      "624/624 [==============================] - 0s 582us/step - loss: 0.6535 - val_loss: 0.6475\n",
      "Epoch 446/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6534 - val_loss: 0.6475\n",
      "Epoch 447/500\n",
      "624/624 [==============================] - 0s 590us/step - loss: 0.6534 - val_loss: 0.6475\n",
      "Epoch 448/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6534 - val_loss: 0.6475\n",
      "Epoch 449/500\n",
      "624/624 [==============================] - 0s 568us/step - loss: 0.6534 - val_loss: 0.6474\n",
      "Epoch 450/500\n",
      "624/624 [==============================] - 0s 550us/step - loss: 0.6533 - val_loss: 0.6474\n",
      "Epoch 451/500\n",
      "624/624 [==============================] - 0s 579us/step - loss: 0.6533 - val_loss: 0.6473\n",
      "Epoch 452/500\n",
      "624/624 [==============================] - 0s 603us/step - loss: 0.6533 - val_loss: 0.6473\n",
      "Epoch 453/500\n",
      "624/624 [==============================] - 0s 583us/step - loss: 0.6532 - val_loss: 0.6473\n",
      "Epoch 454/500\n",
      "624/624 [==============================] - 0s 595us/step - loss: 0.6532 - val_loss: 0.6473\n",
      "Epoch 455/500\n",
      "624/624 [==============================] - 0s 564us/step - loss: 0.6532 - val_loss: 0.6473\n",
      "Epoch 456/500\n",
      "624/624 [==============================] - 0s 599us/step - loss: 0.6531 - val_loss: 0.6473\n",
      "Epoch 457/500\n",
      "624/624 [==============================] - 0s 626us/step - loss: 0.6532 - val_loss: 0.6472\n",
      "Epoch 458/500\n",
      "624/624 [==============================] - 0s 633us/step - loss: 0.6531 - val_loss: 0.6473\n",
      "Epoch 459/500\n",
      "624/624 [==============================] - 0s 630us/step - loss: 0.6531 - val_loss: 0.6473\n",
      "Epoch 460/500\n",
      "624/624 [==============================] - 0s 670us/step - loss: 0.6531 - val_loss: 0.6472\n",
      "Epoch 461/500\n",
      "624/624 [==============================] - 0s 644us/step - loss: 0.6531 - val_loss: 0.6472\n",
      "Epoch 462/500\n",
      "624/624 [==============================] - 0s 664us/step - loss: 0.6530 - val_loss: 0.6472\n",
      "Epoch 463/500\n",
      "624/624 [==============================] - 0s 662us/step - loss: 0.6530 - val_loss: 0.6471\n",
      "Epoch 464/500\n",
      "624/624 [==============================] - 0s 680us/step - loss: 0.6530 - val_loss: 0.6471\n",
      "Epoch 465/500\n",
      "624/624 [==============================] - 0s 694us/step - loss: 0.6529 - val_loss: 0.6471\n",
      "Epoch 466/500\n",
      "624/624 [==============================] - 0s 687us/step - loss: 0.6529 - val_loss: 0.6470\n",
      "Epoch 467/500\n",
      "624/624 [==============================] - 0s 604us/step - loss: 0.6529 - val_loss: 0.6471\n",
      "Epoch 468/500\n",
      "624/624 [==============================] - 0s 609us/step - loss: 0.6528 - val_loss: 0.6470\n",
      "Epoch 469/500\n",
      "624/624 [==============================] - 0s 598us/step - loss: 0.6528 - val_loss: 0.6470\n",
      "Epoch 470/500\n",
      "624/624 [==============================] - 0s 593us/step - loss: 0.6528 - val_loss: 0.6470\n",
      "Epoch 471/500\n",
      "624/624 [==============================] - 0s 600us/step - loss: 0.6528 - val_loss: 0.6471\n",
      "Epoch 472/500\n",
      "624/624 [==============================] - 0s 594us/step - loss: 0.6528 - val_loss: 0.6469\n",
      "Epoch 473/500\n",
      "624/624 [==============================] - 0s 607us/step - loss: 0.6527 - val_loss: 0.6469\n",
      "Epoch 474/500\n",
      "624/624 [==============================] - 0s 623us/step - loss: 0.6527 - val_loss: 0.6469\n",
      "Epoch 475/500\n",
      "624/624 [==============================] - 0s 612us/step - loss: 0.6527 - val_loss: 0.6469\n",
      "Epoch 476/500\n",
      "624/624 [==============================] - 0s 585us/step - loss: 0.6527 - val_loss: 0.6469\n",
      "Epoch 477/500\n",
      "624/624 [==============================] - 0s 597us/step - loss: 0.6527 - val_loss: 0.6469\n",
      "Epoch 478/500\n",
      "624/624 [==============================] - 0s 599us/step - loss: 0.6526 - val_loss: 0.6468\n",
      "Epoch 479/500\n",
      "624/624 [==============================] - 0s 599us/step - loss: 0.6526 - val_loss: 0.6468\n",
      "Epoch 480/500\n",
      "624/624 [==============================] - 0s 610us/step - loss: 0.6526 - val_loss: 0.6468\n",
      "Epoch 481/500\n",
      "624/624 [==============================] - 0s 608us/step - loss: 0.6526 - val_loss: 0.6468\n",
      "Epoch 482/500\n",
      "624/624 [==============================] - 0s 607us/step - loss: 0.6525 - val_loss: 0.6467\n",
      "Epoch 483/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6525 - val_loss: 0.6468\n",
      "Epoch 484/500\n",
      "624/624 [==============================] - 0s 549us/step - loss: 0.6525 - val_loss: 0.6468\n",
      "Epoch 485/500\n",
      "624/624 [==============================] - 0s 551us/step - loss: 0.6525 - val_loss: 0.6468\n",
      "Epoch 486/500\n",
      "624/624 [==============================] - 0s 538us/step - loss: 0.6525 - val_loss: 0.6467\n",
      "Epoch 487/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6524 - val_loss: 0.6467\n",
      "Epoch 488/500\n",
      "624/624 [==============================] - 0s 543us/step - loss: 0.6523 - val_loss: 0.6465\n",
      "Epoch 489/500\n",
      "624/624 [==============================] - 0s 614us/step - loss: 0.6522 - val_loss: 0.6465\n",
      "Epoch 490/500\n",
      "624/624 [==============================] - 0s 542us/step - loss: 0.6521 - val_loss: 0.6466\n",
      "Epoch 491/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6521 - val_loss: 0.6464\n",
      "Epoch 492/500\n",
      "624/624 [==============================] - 0s 548us/step - loss: 0.6521 - val_loss: 0.6464\n",
      "Epoch 493/500\n",
      "624/624 [==============================] - 0s 561us/step - loss: 0.6520 - val_loss: 0.6464\n",
      "Epoch 494/500\n",
      "624/624 [==============================] - 0s 565us/step - loss: 0.6520 - val_loss: 0.6464\n",
      "Epoch 495/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6520 - val_loss: 0.6464\n",
      "Epoch 496/500\n",
      "624/624 [==============================] - 0s 560us/step - loss: 0.6520 - val_loss: 0.6464\n",
      "Epoch 497/500\n",
      "624/624 [==============================] - 0s 557us/step - loss: 0.6520 - val_loss: 0.6464\n",
      "Epoch 498/500\n",
      "624/624 [==============================] - 0s 555us/step - loss: 0.6519 - val_loss: 0.6463\n",
      "Epoch 499/500\n",
      "624/624 [==============================] - 0s 558us/step - loss: 0.6519 - val_loss: 0.6464\n",
      "Epoch 500/500\n",
      "624/624 [==============================] - 0s 550us/step - loss: 0.6519 - val_loss: 0.6462\n"
     ]
    }
   ],
   "source": [
    "# TODO: fit your model here\n",
    "\n",
    "history = autoencoder.fit(X_train_scaled, X_train_scaled, \n",
    "                          epochs=epochs, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          validation_split=0.2,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "Nx592PIi1rnN",
    "outputId": "14a658fc-828b-49e4-cf94-0376361183f2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAJQCAYAAADR8SOKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Plot learning curve of the model\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.xlabel('epochs'), plt.ylabel('Model Loss')\n",
    "    plt.legend(), plt.show()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DXNeIs041rnT",
    "outputId": "ee8ed3f8-7247-41a4-d8cf-904410d06e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 2.574618\n",
      "Test error: 2.533422\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute reconstruction error\n",
    "rec_train = autoencoder.predict(X_train_scaled) * var + mean\n",
    "train_ae_error = loss_function(X_train, rec_train)\n",
    "\n",
    "rec_test = autoencoder.predict(X_test_scaled) * var + mean\n",
    "test_ae_error = loss_function(X_test, rec_test)\n",
    "\n",
    "print(\"Train error: %f\" % train_ae_error)\n",
    "print(\"Test error: %f\" % test_ae_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7znyPWN41rnX"
   },
   "source": [
    "# Bonus: Classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omiyOIdP1rnY"
   },
   "source": [
    "As you already know, there is more than one way to project a data of high dimension to a latent space of reduced dimension. When trying to find a model which can reconstruct the input effectively, the latent data should be able to encode as much data in the input as possible. However, when you are trying to use the latent data for classifying, most likely you don't care if all the data in the input is present. Rather, you only care about relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1khpfo_y1rnY"
   },
   "source": [
    "Create a classifier neural network which takes the output of your encoder as input and predicts its label. Connect this to your model so your architecture looks like the figure below. Train your neural network and report your classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ciA2WDqGsAW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pohLIej1rnZ"
   },
   "source": [
    "<center><img src=\"images/bonus.png\" width=\"20%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "cj5JrVxPaOSv",
    "outputId": "a81aa79c-6d1f-461d-a29d-3799b81d3ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 800)          800800      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 400)          320400      dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 100)          40100       dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 80)           8080        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 30)           2430        dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 400)          40400       dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classifier (Dense)              (None, 16)           496         dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "autoencoder (Dense)             (None, 1000)         401000      dense_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,613,706\n",
      "Trainable params: 1,613,706\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(1000,))\n",
    "\n",
    "# encoder\n",
    "encoded = input_data\n",
    "encoded = Dense(800, activation='relu')(encoded)\n",
    "encoded = Dense(400, activation='relu')(encoded)\n",
    "encoded = Dense(100, activation='relu')(encoded)\n",
    "\n",
    "# classifier\n",
    "fc = Dense(80,  activation='relu')(encoded)\n",
    "fc = Dense(30,  activation='relu')(fc)\n",
    "softmax = Dense(n_classes, activation='softmax', name='classifier')(fc)\n",
    "\n",
    "# decoder\n",
    "decoded = encoded\n",
    "encoded = Dense(100)(encoded)\n",
    "decoded = Dense(400, activation='relu')(decoded)\n",
    "encoded = Dense(800, activation='relu')(encoded)\n",
    "decoded = Dense(1000, activation='sigmoid', name='autoencoder')(decoded)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=[softmax, decoded])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3454
    },
    "colab_type": "code",
    "id": "jf0esMjadtF5",
    "outputId": "0cc9f879-ed32-4d7e-e46f-74ec672bd04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 624 samples, validate on 156 samples\n",
      "Epoch 1/100\n",
      "624/624 [==============================] - 2s 3ms/step - loss: 1.5262 - classifier_loss: 0.5604 - autoencoder_loss: 0.9658 - classifier_acc: 0.8750 - val_loss: 1.0615 - val_classifier_loss: 0.2427 - val_autoencoder_loss: 0.8188 - val_classifier_acc: 0.9103\n",
      "Epoch 2/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.8550 - classifier_loss: 0.1010 - autoencoder_loss: 0.7540 - classifier_acc: 0.9776 - val_loss: 0.8480 - val_classifier_loss: 0.1634 - val_autoencoder_loss: 0.6846 - val_classifier_acc: 0.9615\n",
      "Epoch 3/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.7069 - classifier_loss: 0.0422 - autoencoder_loss: 0.6647 - classifier_acc: 0.9952 - val_loss: 0.6375 - val_classifier_loss: 9.0369e-04 - val_autoencoder_loss: 0.6366 - val_classifier_acc: 1.0000\n",
      "Epoch 4/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6618 - classifier_loss: 0.0264 - autoencoder_loss: 0.6355 - classifier_acc: 0.9984 - val_loss: 0.6213 - val_classifier_loss: 3.6555e-04 - val_autoencoder_loss: 0.6209 - val_classifier_acc: 1.0000\n",
      "Epoch 5/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6509 - classifier_loss: 0.0261 - autoencoder_loss: 0.6248 - classifier_acc: 0.9984 - val_loss: 0.6159 - val_classifier_loss: 2.3915e-04 - val_autoencoder_loss: 0.6156 - val_classifier_acc: 1.0000\n",
      "Epoch 6/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6441 - classifier_loss: 0.0260 - autoencoder_loss: 0.6181 - classifier_acc: 0.9984 - val_loss: 0.6112 - val_classifier_loss: 1.6197e-04 - val_autoencoder_loss: 0.6111 - val_classifier_acc: 1.0000\n",
      "Epoch 7/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6392 - classifier_loss: 0.0260 - autoencoder_loss: 0.6133 - classifier_acc: 0.9984 - val_loss: 0.6064 - val_classifier_loss: 1.2922e-04 - val_autoencoder_loss: 0.6063 - val_classifier_acc: 1.0000\n",
      "Epoch 8/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6351 - classifier_loss: 0.0259 - autoencoder_loss: 0.6091 - classifier_acc: 0.9984 - val_loss: 0.6037 - val_classifier_loss: 9.6783e-05 - val_autoencoder_loss: 0.6036 - val_classifier_acc: 1.0000\n",
      "Epoch 9/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6313 - classifier_loss: 0.0259 - autoencoder_loss: 0.6054 - classifier_acc: 0.9984 - val_loss: 0.6011 - val_classifier_loss: 7.6087e-05 - val_autoencoder_loss: 0.6010 - val_classifier_acc: 1.0000\n",
      "Epoch 10/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6283 - classifier_loss: 0.0259 - autoencoder_loss: 0.6024 - classifier_acc: 0.9984 - val_loss: 0.5983 - val_classifier_loss: 6.1981e-05 - val_autoencoder_loss: 0.5982 - val_classifier_acc: 1.0000\n",
      "Epoch 11/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6251 - classifier_loss: 0.0259 - autoencoder_loss: 0.5992 - classifier_acc: 0.9984 - val_loss: 0.5956 - val_classifier_loss: 5.2099e-05 - val_autoencoder_loss: 0.5956 - val_classifier_acc: 1.0000\n",
      "Epoch 12/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6234 - classifier_loss: 0.0259 - autoencoder_loss: 0.5975 - classifier_acc: 0.9984 - val_loss: 0.5944 - val_classifier_loss: 4.7570e-05 - val_autoencoder_loss: 0.5944 - val_classifier_acc: 1.0000\n",
      "Epoch 13/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6209 - classifier_loss: 0.0259 - autoencoder_loss: 0.5950 - classifier_acc: 0.9984 - val_loss: 0.5922 - val_classifier_loss: 4.0690e-05 - val_autoencoder_loss: 0.5921 - val_classifier_acc: 1.0000\n",
      "Epoch 14/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6195 - classifier_loss: 0.0259 - autoencoder_loss: 0.5936 - classifier_acc: 0.9984 - val_loss: 0.5911 - val_classifier_loss: 3.5241e-05 - val_autoencoder_loss: 0.5910 - val_classifier_acc: 1.0000\n",
      "Epoch 15/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6176 - classifier_loss: 0.0259 - autoencoder_loss: 0.5918 - classifier_acc: 0.9984 - val_loss: 0.5897 - val_classifier_loss: 3.2005e-05 - val_autoencoder_loss: 0.5897 - val_classifier_acc: 1.0000\n",
      "Epoch 16/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6157 - classifier_loss: 0.0259 - autoencoder_loss: 0.5898 - classifier_acc: 0.9984 - val_loss: 0.5878 - val_classifier_loss: 2.6442e-05 - val_autoencoder_loss: 0.5878 - val_classifier_acc: 1.0000\n",
      "Epoch 17/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6135 - classifier_loss: 0.0259 - autoencoder_loss: 0.5876 - classifier_acc: 0.9984 - val_loss: 0.5857 - val_classifier_loss: 2.3078e-05 - val_autoencoder_loss: 0.5857 - val_classifier_acc: 1.0000\n",
      "Epoch 18/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6119 - classifier_loss: 0.0259 - autoencoder_loss: 0.5861 - classifier_acc: 0.9984 - val_loss: 0.5848 - val_classifier_loss: 2.0468e-05 - val_autoencoder_loss: 0.5848 - val_classifier_acc: 1.0000\n",
      "Epoch 19/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6107 - classifier_loss: 0.0259 - autoencoder_loss: 0.5848 - classifier_acc: 0.9984 - val_loss: 0.5836 - val_classifier_loss: 1.6041e-05 - val_autoencoder_loss: 0.5836 - val_classifier_acc: 1.0000\n",
      "Epoch 20/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6091 - classifier_loss: 0.0259 - autoencoder_loss: 0.5833 - classifier_acc: 0.9984 - val_loss: 0.5813 - val_classifier_loss: 1.6518e-05 - val_autoencoder_loss: 0.5813 - val_classifier_acc: 1.0000\n",
      "Epoch 21/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6075 - classifier_loss: 0.0259 - autoencoder_loss: 0.5817 - classifier_acc: 0.9984 - val_loss: 0.5805 - val_classifier_loss: 1.2453e-05 - val_autoencoder_loss: 0.5805 - val_classifier_acc: 1.0000\n",
      "Epoch 22/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6065 - classifier_loss: 0.0259 - autoencoder_loss: 0.5806 - classifier_acc: 0.9984 - val_loss: 0.5796 - val_classifier_loss: 1.1825e-05 - val_autoencoder_loss: 0.5796 - val_classifier_acc: 1.0000\n",
      "Epoch 23/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6054 - classifier_loss: 0.0259 - autoencoder_loss: 0.5796 - classifier_acc: 0.9984 - val_loss: 0.5785 - val_classifier_loss: 1.1216e-05 - val_autoencoder_loss: 0.5785 - val_classifier_acc: 1.0000\n",
      "Epoch 24/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6046 - classifier_loss: 0.0258 - autoencoder_loss: 0.5787 - classifier_acc: 0.9984 - val_loss: 0.5777 - val_classifier_loss: 1.0081e-05 - val_autoencoder_loss: 0.5777 - val_classifier_acc: 1.0000\n",
      "Epoch 25/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6039 - classifier_loss: 0.0258 - autoencoder_loss: 0.5781 - classifier_acc: 0.9984 - val_loss: 0.5776 - val_classifier_loss: 1.0080e-05 - val_autoencoder_loss: 0.5776 - val_classifier_acc: 1.0000\n",
      "Epoch 26/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6031 - classifier_loss: 0.0258 - autoencoder_loss: 0.5773 - classifier_acc: 0.9984 - val_loss: 0.5769 - val_classifier_loss: 9.6189e-06 - val_autoencoder_loss: 0.5768 - val_classifier_acc: 1.0000\n",
      "Epoch 27/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6021 - classifier_loss: 0.0258 - autoencoder_loss: 0.5763 - classifier_acc: 0.9984 - val_loss: 0.5761 - val_classifier_loss: 9.1425e-06 - val_autoencoder_loss: 0.5761 - val_classifier_acc: 1.0000\n",
      "Epoch 28/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6012 - classifier_loss: 0.0258 - autoencoder_loss: 0.5754 - classifier_acc: 0.9984 - val_loss: 0.5752 - val_classifier_loss: 8.4882e-06 - val_autoencoder_loss: 0.5752 - val_classifier_acc: 1.0000\n",
      "Epoch 29/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.6006 - classifier_loss: 0.0258 - autoencoder_loss: 0.5747 - classifier_acc: 0.9984 - val_loss: 0.5742 - val_classifier_loss: 8.0744e-06 - val_autoencoder_loss: 0.5742 - val_classifier_acc: 1.0000\n",
      "Epoch 30/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5998 - classifier_loss: 0.0258 - autoencoder_loss: 0.5739 - classifier_acc: 0.9984 - val_loss: 0.5738 - val_classifier_loss: 8.3362e-06 - val_autoencoder_loss: 0.5738 - val_classifier_acc: 1.0000\n",
      "Epoch 31/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5992 - classifier_loss: 0.0258 - autoencoder_loss: 0.5733 - classifier_acc: 0.9984 - val_loss: 0.5731 - val_classifier_loss: 7.2456e-06 - val_autoencoder_loss: 0.5731 - val_classifier_acc: 1.0000\n",
      "Epoch 32/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5984 - classifier_loss: 0.0258 - autoencoder_loss: 0.5726 - classifier_acc: 0.9984 - val_loss: 0.5724 - val_classifier_loss: 7.1627e-06 - val_autoencoder_loss: 0.5724 - val_classifier_acc: 1.0000\n",
      "Epoch 33/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5979 - classifier_loss: 0.0258 - autoencoder_loss: 0.5720 - classifier_acc: 0.9984 - val_loss: 0.5720 - val_classifier_loss: 6.5425e-06 - val_autoencoder_loss: 0.5720 - val_classifier_acc: 1.0000\n",
      "Epoch 34/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5974 - classifier_loss: 0.0258 - autoencoder_loss: 0.5715 - classifier_acc: 0.9984 - val_loss: 0.5715 - val_classifier_loss: 6.5490e-06 - val_autoencoder_loss: 0.5715 - val_classifier_acc: 1.0000\n",
      "Epoch 35/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5967 - classifier_loss: 0.0258 - autoencoder_loss: 0.5708 - classifier_acc: 0.9984 - val_loss: 0.5708 - val_classifier_loss: 6.9289e-06 - val_autoencoder_loss: 0.5707 - val_classifier_acc: 1.0000\n",
      "Epoch 36/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5959 - classifier_loss: 0.0258 - autoencoder_loss: 0.5701 - classifier_acc: 0.9984 - val_loss: 0.5705 - val_classifier_loss: 6.0791e-06 - val_autoencoder_loss: 0.5705 - val_classifier_acc: 1.0000\n",
      "Epoch 37/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5955 - classifier_loss: 0.0258 - autoencoder_loss: 0.5697 - classifier_acc: 0.9984 - val_loss: 0.5701 - val_classifier_loss: 5.7451e-06 - val_autoencoder_loss: 0.5701 - val_classifier_acc: 1.0000\n",
      "Epoch 38/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5952 - classifier_loss: 0.0258 - autoencoder_loss: 0.5693 - classifier_acc: 0.9984 - val_loss: 0.5699 - val_classifier_loss: 5.8253e-06 - val_autoencoder_loss: 0.5699 - val_classifier_acc: 1.0000\n",
      "Epoch 39/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5946 - classifier_loss: 0.0258 - autoencoder_loss: 0.5688 - classifier_acc: 0.9984 - val_loss: 0.5693 - val_classifier_loss: 5.2407e-06 - val_autoencoder_loss: 0.5693 - val_classifier_acc: 1.0000\n",
      "Epoch 40/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5941 - classifier_loss: 0.0258 - autoencoder_loss: 0.5683 - classifier_acc: 0.9984 - val_loss: 0.5684 - val_classifier_loss: 5.5869e-06 - val_autoencoder_loss: 0.5684 - val_classifier_acc: 1.0000\n",
      "Epoch 41/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5934 - classifier_loss: 0.0258 - autoencoder_loss: 0.5676 - classifier_acc: 0.9984 - val_loss: 0.5683 - val_classifier_loss: 4.9129e-06 - val_autoencoder_loss: 0.5683 - val_classifier_acc: 1.0000\n",
      "Epoch 42/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5928 - classifier_loss: 0.0258 - autoencoder_loss: 0.5669 - classifier_acc: 0.9984 - val_loss: 0.5675 - val_classifier_loss: 4.6569e-06 - val_autoencoder_loss: 0.5675 - val_classifier_acc: 1.0000\n",
      "Epoch 43/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5921 - classifier_loss: 0.0258 - autoencoder_loss: 0.5662 - classifier_acc: 0.9984 - val_loss: 0.5671 - val_classifier_loss: 4.4785e-06 - val_autoencoder_loss: 0.5671 - val_classifier_acc: 1.0000\n",
      "Epoch 44/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5917 - classifier_loss: 0.0258 - autoencoder_loss: 0.5658 - classifier_acc: 0.9984 - val_loss: 0.5670 - val_classifier_loss: 4.3382e-06 - val_autoencoder_loss: 0.5670 - val_classifier_acc: 1.0000\n",
      "Epoch 45/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5914 - classifier_loss: 0.0258 - autoencoder_loss: 0.5655 - classifier_acc: 0.9984 - val_loss: 0.5668 - val_classifier_loss: 4.0849e-06 - val_autoencoder_loss: 0.5668 - val_classifier_acc: 1.0000\n",
      "Epoch 46/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5913 - classifier_loss: 0.0258 - autoencoder_loss: 0.5655 - classifier_acc: 0.9984 - val_loss: 0.5668 - val_classifier_loss: 3.8816e-06 - val_autoencoder_loss: 0.5668 - val_classifier_acc: 1.0000\n",
      "Epoch 47/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5908 - classifier_loss: 0.0258 - autoencoder_loss: 0.5650 - classifier_acc: 0.9984 - val_loss: 0.5662 - val_classifier_loss: 4.1888e-06 - val_autoencoder_loss: 0.5662 - val_classifier_acc: 1.0000\n",
      "Epoch 48/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5907 - classifier_loss: 0.0258 - autoencoder_loss: 0.5648 - classifier_acc: 0.9984 - val_loss: 0.5664 - val_classifier_loss: 4.1877e-06 - val_autoencoder_loss: 0.5664 - val_classifier_acc: 1.0000\n",
      "Epoch 49/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5903 - classifier_loss: 0.0258 - autoencoder_loss: 0.5645 - classifier_acc: 0.9984 - val_loss: 0.5662 - val_classifier_loss: 3.9210e-06 - val_autoencoder_loss: 0.5662 - val_classifier_acc: 1.0000\n",
      "Epoch 50/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5898 - classifier_loss: 0.0258 - autoencoder_loss: 0.5640 - classifier_acc: 0.9984 - val_loss: 0.5658 - val_classifier_loss: 4.3336e-06 - val_autoencoder_loss: 0.5658 - val_classifier_acc: 1.0000\n",
      "Epoch 51/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5893 - classifier_loss: 0.0258 - autoencoder_loss: 0.5635 - classifier_acc: 0.9984 - val_loss: 0.5653 - val_classifier_loss: 3.5886e-06 - val_autoencoder_loss: 0.5653 - val_classifier_acc: 1.0000\n",
      "Epoch 52/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5892 - classifier_loss: 0.0258 - autoencoder_loss: 0.5633 - classifier_acc: 0.9984 - val_loss: 0.5654 - val_classifier_loss: 3.4346e-06 - val_autoencoder_loss: 0.5654 - val_classifier_acc: 1.0000\n",
      "Epoch 53/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5887 - classifier_loss: 0.0258 - autoencoder_loss: 0.5629 - classifier_acc: 0.9984 - val_loss: 0.5652 - val_classifier_loss: 3.3834e-06 - val_autoencoder_loss: 0.5652 - val_classifier_acc: 1.0000\n",
      "Epoch 54/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5886 - classifier_loss: 0.0258 - autoencoder_loss: 0.5627 - classifier_acc: 0.9984 - val_loss: 0.5653 - val_classifier_loss: 3.4457e-06 - val_autoencoder_loss: 0.5653 - val_classifier_acc: 1.0000\n",
      "Epoch 55/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5885 - classifier_loss: 0.0258 - autoencoder_loss: 0.5627 - classifier_acc: 0.9984 - val_loss: 0.5655 - val_classifier_loss: 4.3734e-06 - val_autoencoder_loss: 0.5655 - val_classifier_acc: 1.0000\n",
      "Epoch 56/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5882 - classifier_loss: 0.0258 - autoencoder_loss: 0.5623 - classifier_acc: 0.9984 - val_loss: 0.5652 - val_classifier_loss: 3.2153e-06 - val_autoencoder_loss: 0.5652 - val_classifier_acc: 1.0000\n",
      "Epoch 57/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5878 - classifier_loss: 0.0258 - autoencoder_loss: 0.5620 - classifier_acc: 0.9984 - val_loss: 0.5647 - val_classifier_loss: 3.7303e-06 - val_autoencoder_loss: 0.5647 - val_classifier_acc: 1.0000\n",
      "Epoch 58/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5872 - classifier_loss: 0.0258 - autoencoder_loss: 0.5614 - classifier_acc: 0.9984 - val_loss: 0.5650 - val_classifier_loss: 3.6940e-06 - val_autoencoder_loss: 0.5650 - val_classifier_acc: 1.0000\n",
      "Epoch 59/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5871 - classifier_loss: 0.0258 - autoencoder_loss: 0.5613 - classifier_acc: 0.9984 - val_loss: 0.5645 - val_classifier_loss: 3.4812e-06 - val_autoencoder_loss: 0.5645 - val_classifier_acc: 1.0000\n",
      "Epoch 60/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5866 - classifier_loss: 0.0258 - autoencoder_loss: 0.5608 - classifier_acc: 0.9984 - val_loss: 0.5639 - val_classifier_loss: 3.2302e-06 - val_autoencoder_loss: 0.5639 - val_classifier_acc: 1.0000\n",
      "Epoch 61/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5862 - classifier_loss: 0.0258 - autoencoder_loss: 0.5604 - classifier_acc: 0.9984 - val_loss: 0.5636 - val_classifier_loss: 3.0284e-06 - val_autoencoder_loss: 0.5636 - val_classifier_acc: 1.0000\n",
      "Epoch 62/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5863 - classifier_loss: 0.0258 - autoencoder_loss: 0.5605 - classifier_acc: 0.9984 - val_loss: 0.5637 - val_classifier_loss: 2.8607e-06 - val_autoencoder_loss: 0.5637 - val_classifier_acc: 1.0000\n",
      "Epoch 63/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5861 - classifier_loss: 0.0258 - autoencoder_loss: 0.5602 - classifier_acc: 0.9984 - val_loss: 0.5637 - val_classifier_loss: 3.0078e-06 - val_autoencoder_loss: 0.5637 - val_classifier_acc: 1.0000\n",
      "Epoch 64/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5857 - classifier_loss: 0.0258 - autoencoder_loss: 0.5599 - classifier_acc: 0.9984 - val_loss: 0.5632 - val_classifier_loss: 3.2374e-06 - val_autoencoder_loss: 0.5632 - val_classifier_acc: 1.0000\n",
      "Epoch 65/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5854 - classifier_loss: 0.0258 - autoencoder_loss: 0.5596 - classifier_acc: 0.9984 - val_loss: 0.5632 - val_classifier_loss: 3.1182e-06 - val_autoencoder_loss: 0.5632 - val_classifier_acc: 1.0000\n",
      "Epoch 66/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5852 - classifier_loss: 0.0258 - autoencoder_loss: 0.5594 - classifier_acc: 0.9984 - val_loss: 0.5630 - val_classifier_loss: 2.8477e-06 - val_autoencoder_loss: 0.5630 - val_classifier_acc: 1.0000\n",
      "Epoch 67/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5852 - classifier_loss: 0.0258 - autoencoder_loss: 0.5593 - classifier_acc: 0.9984 - val_loss: 0.5634 - val_classifier_loss: 2.9237e-06 - val_autoencoder_loss: 0.5634 - val_classifier_acc: 1.0000\n",
      "Epoch 68/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5858 - classifier_loss: 0.0258 - autoencoder_loss: 0.5600 - classifier_acc: 0.9984 - val_loss: 0.5639 - val_classifier_loss: 4.1045e-06 - val_autoencoder_loss: 0.5639 - val_classifier_acc: 1.0000\n",
      "Epoch 69/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5859 - classifier_loss: 0.0258 - autoencoder_loss: 0.5600 - classifier_acc: 0.9984 - val_loss: 0.5632 - val_classifier_loss: 2.9593e-06 - val_autoencoder_loss: 0.5632 - val_classifier_acc: 1.0000\n",
      "Epoch 70/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5853 - classifier_loss: 0.0258 - autoencoder_loss: 0.5594 - classifier_acc: 0.9984 - val_loss: 0.5629 - val_classifier_loss: 2.5581e-06 - val_autoencoder_loss: 0.5629 - val_classifier_acc: 1.0000\n",
      "Epoch 71/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5842 - classifier_loss: 0.0258 - autoencoder_loss: 0.5584 - classifier_acc: 0.9984 - val_loss: 0.5623 - val_classifier_loss: 2.4117e-06 - val_autoencoder_loss: 0.5623 - val_classifier_acc: 1.0000\n",
      "Epoch 72/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5840 - classifier_loss: 0.0258 - autoencoder_loss: 0.5582 - classifier_acc: 0.9984 - val_loss: 0.5626 - val_classifier_loss: 2.7227e-06 - val_autoencoder_loss: 0.5626 - val_classifier_acc: 1.0000\n",
      "Epoch 73/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5838 - classifier_loss: 0.0258 - autoencoder_loss: 0.5580 - classifier_acc: 0.9984 - val_loss: 0.5625 - val_classifier_loss: 2.7793e-06 - val_autoencoder_loss: 0.5625 - val_classifier_acc: 1.0000\n",
      "Epoch 74/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5835 - classifier_loss: 0.0258 - autoencoder_loss: 0.5576 - classifier_acc: 0.9984 - val_loss: 0.5621 - val_classifier_loss: 2.7117e-06 - val_autoencoder_loss: 0.5621 - val_classifier_acc: 1.0000\n",
      "Epoch 75/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5834 - classifier_loss: 0.0258 - autoencoder_loss: 0.5576 - classifier_acc: 0.9984 - val_loss: 0.5620 - val_classifier_loss: 2.6761e-06 - val_autoencoder_loss: 0.5620 - val_classifier_acc: 1.0000\n",
      "Epoch 76/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5836 - classifier_loss: 0.0258 - autoencoder_loss: 0.5577 - classifier_acc: 0.9984 - val_loss: 0.5624 - val_classifier_loss: 2.4018e-06 - val_autoencoder_loss: 0.5624 - val_classifier_acc: 1.0000\n",
      "Epoch 77/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5834 - classifier_loss: 0.0258 - autoencoder_loss: 0.5576 - classifier_acc: 0.9984 - val_loss: 0.5627 - val_classifier_loss: 2.3464e-06 - val_autoencoder_loss: 0.5627 - val_classifier_acc: 1.0000\n",
      "Epoch 78/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5833 - classifier_loss: 0.0258 - autoencoder_loss: 0.5575 - classifier_acc: 0.9984 - val_loss: 0.5617 - val_classifier_loss: 2.2707e-06 - val_autoencoder_loss: 0.5617 - val_classifier_acc: 1.0000\n",
      "Epoch 79/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5826 - classifier_loss: 0.0258 - autoencoder_loss: 0.5568 - classifier_acc: 0.9984 - val_loss: 0.5616 - val_classifier_loss: 2.2967e-06 - val_autoencoder_loss: 0.5616 - val_classifier_acc: 1.0000\n",
      "Epoch 80/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5824 - classifier_loss: 0.0258 - autoencoder_loss: 0.5566 - classifier_acc: 0.9984 - val_loss: 0.5615 - val_classifier_loss: 1.9173e-06 - val_autoencoder_loss: 0.5614 - val_classifier_acc: 1.0000\n",
      "Epoch 81/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5824 - classifier_loss: 0.0258 - autoencoder_loss: 0.5565 - classifier_acc: 0.9984 - val_loss: 0.5616 - val_classifier_loss: 2.1099e-06 - val_autoencoder_loss: 0.5616 - val_classifier_acc: 1.0000\n",
      "Epoch 82/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5820 - classifier_loss: 0.0258 - autoencoder_loss: 0.5562 - classifier_acc: 0.9984 - val_loss: 0.5610 - val_classifier_loss: 2.0059e-06 - val_autoencoder_loss: 0.5610 - val_classifier_acc: 1.0000\n",
      "Epoch 83/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5819 - classifier_loss: 0.0258 - autoencoder_loss: 0.5561 - classifier_acc: 0.9984 - val_loss: 0.5615 - val_classifier_loss: 1.7904e-06 - val_autoencoder_loss: 0.5615 - val_classifier_acc: 1.0000\n",
      "Epoch 84/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5816 - classifier_loss: 0.0258 - autoencoder_loss: 0.5557 - classifier_acc: 0.9984 - val_loss: 0.5610 - val_classifier_loss: 1.6487e-06 - val_autoencoder_loss: 0.5610 - val_classifier_acc: 1.0000\n",
      "Epoch 85/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5815 - classifier_loss: 0.0258 - autoencoder_loss: 0.5556 - classifier_acc: 0.9984 - val_loss: 0.5614 - val_classifier_loss: 2.1213e-06 - val_autoencoder_loss: 0.5614 - val_classifier_acc: 1.0000\n",
      "Epoch 86/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5815 - classifier_loss: 0.0258 - autoencoder_loss: 0.5556 - classifier_acc: 0.9984 - val_loss: 0.5612 - val_classifier_loss: 1.6816e-06 - val_autoencoder_loss: 0.5612 - val_classifier_acc: 1.0000\n",
      "Epoch 87/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5815 - classifier_loss: 0.0258 - autoencoder_loss: 0.5557 - classifier_acc: 0.9984 - val_loss: 0.5610 - val_classifier_loss: 1.5387e-06 - val_autoencoder_loss: 0.5610 - val_classifier_acc: 1.0000\n",
      "Epoch 88/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5812 - classifier_loss: 0.0258 - autoencoder_loss: 0.5554 - classifier_acc: 0.9984 - val_loss: 0.5608 - val_classifier_loss: 1.4504e-06 - val_autoencoder_loss: 0.5608 - val_classifier_acc: 1.0000\n",
      "Epoch 89/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5810 - classifier_loss: 0.0258 - autoencoder_loss: 0.5551 - classifier_acc: 0.9984 - val_loss: 0.5609 - val_classifier_loss: 1.6869e-06 - val_autoencoder_loss: 0.5609 - val_classifier_acc: 1.0000\n",
      "Epoch 90/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5811 - classifier_loss: 0.0258 - autoencoder_loss: 0.5552 - classifier_acc: 0.9984 - val_loss: 0.5608 - val_classifier_loss: 1.5371e-06 - val_autoencoder_loss: 0.5608 - val_classifier_acc: 1.0000\n",
      "Epoch 91/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5808 - classifier_loss: 0.0258 - autoencoder_loss: 0.5550 - classifier_acc: 0.9984 - val_loss: 0.5609 - val_classifier_loss: 1.8596e-06 - val_autoencoder_loss: 0.5609 - val_classifier_acc: 1.0000\n",
      "Epoch 92/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5808 - classifier_loss: 0.0258 - autoencoder_loss: 0.5549 - classifier_acc: 0.9984 - val_loss: 0.5609 - val_classifier_loss: 1.5230e-06 - val_autoencoder_loss: 0.5609 - val_classifier_acc: 1.0000\n",
      "Epoch 93/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5807 - classifier_loss: 0.0258 - autoencoder_loss: 0.5549 - classifier_acc: 0.9984 - val_loss: 0.5610 - val_classifier_loss: 1.6529e-06 - val_autoencoder_loss: 0.5610 - val_classifier_acc: 1.0000\n",
      "Epoch 94/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5809 - classifier_loss: 0.0258 - autoencoder_loss: 0.5551 - classifier_acc: 0.9984 - val_loss: 0.5609 - val_classifier_loss: 1.8757e-06 - val_autoencoder_loss: 0.5609 - val_classifier_acc: 1.0000\n",
      "Epoch 95/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5808 - classifier_loss: 0.0258 - autoencoder_loss: 0.5549 - classifier_acc: 0.9984 - val_loss: 0.5614 - val_classifier_loss: 1.5983e-06 - val_autoencoder_loss: 0.5614 - val_classifier_acc: 1.0000\n",
      "Epoch 96/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5809 - classifier_loss: 0.0258 - autoencoder_loss: 0.5550 - classifier_acc: 0.9984 - val_loss: 0.5613 - val_classifier_loss: 1.9452e-06 - val_autoencoder_loss: 0.5613 - val_classifier_acc: 1.0000\n",
      "Epoch 97/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5808 - classifier_loss: 0.0258 - autoencoder_loss: 0.5550 - classifier_acc: 0.9984 - val_loss: 0.5611 - val_classifier_loss: 2.1771e-06 - val_autoencoder_loss: 0.5611 - val_classifier_acc: 1.0000\n",
      "Epoch 98/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5805 - classifier_loss: 0.0258 - autoencoder_loss: 0.5547 - classifier_acc: 0.9984 - val_loss: 0.5607 - val_classifier_loss: 1.9742e-06 - val_autoencoder_loss: 0.5607 - val_classifier_acc: 1.0000\n",
      "Epoch 99/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5803 - classifier_loss: 0.0258 - autoencoder_loss: 0.5545 - classifier_acc: 0.9984 - val_loss: 0.5609 - val_classifier_loss: 2.1814e-06 - val_autoencoder_loss: 0.5609 - val_classifier_acc: 1.0000\n",
      "Epoch 100/100\n",
      "624/624 [==============================] - 1s 2ms/step - loss: 0.5799 - classifier_loss: 0.0258 - autoencoder_loss: 0.5540 - classifier_acc: 0.9984 - val_loss: 0.5603 - val_classifier_loss: 2.6594e-06 - val_autoencoder_loss: 0.5603 - val_classifier_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss={'classifier': 'categorical_crossentropy', 'autoencoder': 'mse'},\n",
    "              optimizer='adam',\n",
    "              metrics={'classifier': 'acc'})\n",
    "\n",
    "history = model.fit(X_train_scaled, \n",
    "                    {'classifier': y_train, 'autoencoder': X_train_scaled},\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.2, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "colab_type": "code",
    "id": "G6UZnWEBCbuC",
    "outputId": "9e19fd9e-a2aa-42df-db11-5ed71a49de56"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhYKIiFQoFEREpEKhICIiFf8fraWMQc8zGlEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the learning curve\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['classifier_acc'], label='train')\n",
    "    plt.plot(history.history['val_classifier_acc'], label='validation')\n",
    "    plt.xlabel('epochs'), plt.ylabel('Model Accuracy')\n",
    "    plt.legend(), plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.xlabel('epochs'), plt.ylabel('Model Loss')\n",
    "    plt.legend(), plt.show()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "UMPaw9EYxqJ8",
    "outputId": "31422dec-28a1-4f91-e901-1dc23afc404a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - 0s 217us/step\n",
      "260/260 [==============================] - 0s 227us/step\n",
      "Train accuracy: 0.9987179487179487\n",
      "Test accuracy: 0.9961538461538462\n"
     ]
    }
   ],
   "source": [
    "train_acc = model.evaluate(X_train_scaled, {'classifier': y_train, 'autoencoder': X_train_scaled})\n",
    "test_acc = model.evaluate(X_test_scaled, {'classifier': y_test, 'autoencoder': X_test_scaled})\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc[3]))\n",
    "print(\"Test accuracy: {}\".format(test_acc[3]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW5.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
